[{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/tags/back-to-basic/","section":"Tags","summary":"","title":"Back to Basic","type":"tags"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/tags/backup/","section":"Tags","summary":"","title":"Backup","type":"tags"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/tags/plakar/","section":"Tags","summary":"","title":"Plakar","type":"tags"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/authors/plakup/","section":"Authors","summary":"","title":"Plakup","type":"authors"},{"content":"In today\u0026rsquo;s digital world, data resilience is crucial. Our Insights section is dedicated to providing expert guidance on backup best practices, disaster recovery strategies, and emerging trends in data protection.\nWhether you\u0026rsquo;re looking to understand fundamental principles like the 3-2-1 backup rule, explore advanced backup solutions, or learn how to safeguard your data from cyber threats, you\u0026rsquo;ll find valuable insights here.\nStay informed with our in-depth analyses, best practice recommendations, and industry trends to help you build a robust and secure backup strategy.\n📌 Browse our latest articles below and take your backup strategy to the next level! 🚀\n","date":"10 February 2025","externalUrl":null,"permalink":"/insights/","section":"Ready to get some insights?","summary":"In today\u0026rsquo;s digital world, data resilience is crucial.","title":"Ready to get some insights?","type":"insights"},{"content":"Let us get straight to the point: Amazon S3 is a phenomenal service for scalable, reliable object storage but it is not a backup solution. Sure, S3 boasts rock-solid durability and cost efficiency, but relying on it alone for backups is like trying to cover your bases with duct tape. In today’s world, where a single misclick can spell disaster, a thoughtful, multi-layered backup strategy is not just nice to have; it is absolutely essential.\nThis article digs into the reasons why S3’s native features do not suffice when it comes to safeguarding your data. We expose the design limitations of S3 for backup tasks, compare it with dedicated backup solutions, and highlight real-world scenarios that illustrate these challenges. Along the way, we share best practices, practical examples, and a few tongue-in-cheek observations about the perils of relying on S3 as your one-and-only data safeguard. If you are serious about data protection, prepare to rethink your backup strategy.\nUnderstanding S3: its strengths and its intended purpose # Amazon S3 was built to be a high-availability, scalable object storage service. It is designed to handle immense data loads for applications that demand immediate access. It is brilliant at what it does, but it was never designed to be the all-in-one solution for backups.\nWhat is Amazon S3? # At its core, S3 is an object storage system. You drop files into buckets and retrieve them whenever you need them. Its architecture is optimized for durability by distributing data across multiple physical sites. In other words, if a hard drive fails in one location, your data remains safe elsewhere. However, this setup is intended for live data access and distribution, not for managing the nuanced requirements of backups.\nS3’s features such as lifecycle policies, access control lists, and even versioning are powerful, yet they are not built for the kind of point-in-time recoveries or granular data management that a true backup solution demands. S3’s design prioritizes scale and accessibility over the precision and control that backups require. It is like using a fire hose to water your garden: effective for one purpose, but not ideal for another.\nWhy S3 is not meant to be a backup # The reality is that S3 was never designed with a backup mindset. When backing up data, you are not just storing files; you are preparing for worst-case scenarios such as accidental deletion, malicious actions, or even regional disasters. For example, S3’s eventual consistency model means that changes might not immediately reflect across all copies. In a backup scenario, that delay can turn a near-instant restore into a waiting game that could cost you dearly.\nMoreover, S3’s versioning, while useful for retrieving older copies, is not foolproof. If all versions are deleted at once, you are in trouble. Additionally, features like MFA delete make the process of removing unwanted files cumbersome, and Object Lock can restrict deletion permanently, which is not always desirable. S3 was built to store data reliably, not to manage the intricacies of a backup cycle.\nMany cloud services tout extreme durability, but it is important to remember that durability is not the same as recoverability. S3 excels at keeping your data safe from hardware failures, but it does not protect you from human error, configuration mistakes, or targeted attacks. This is why you need a backup strategy that addresses these challenges.\nThe real-world limitations of using S3 as a backup # Relying on S3 as your sole backup solution is a risky proposition. It is not that S3 loses data; rather, it is not built for the specific challenges of backup and recovery. Let us examine the practical limitations.\nData consistency and recovery challenges # Imagine you accidentally delete a critical file and expect S3’s versioning to rescue you. In theory, it might help, but S3 uses an eventual consistency model for certain operations. This means that immediately after a change, not all copies of your data may be updated. In a scenario where every second matters, this delay can lead to inconsistencies in the recovered data.\nConsider a situation where an application update inadvertently overwrites the latest version of a critical file. With S3, versioning might help, but only if you can roll back quickly and if the previous version is intact. More often than not, recovery becomes a tedious process. Recovery is not just about retrieving the latest copy of a file; it is about ensuring every piece of data is exactly where it should be, a task S3 is not optimized for.\nSecurity and compliance limitations # When protecting vital data, security is not optional; it is a mandate. Although S3 supports encryption and access control, setting these features up correctly can be challenging. A minor misconfiguration may leave your backup data exposed to malicious actors. Traditional backup solutions are designed with integrated security protocols that ensure data remains encrypted both in transit and at rest with minimal effort. S3, on the other hand, requires continuous attention to maintain proper security.\nCompliance is another concern. For industries with strict regulatory requirements, S3’s native security settings may not be sufficient. Standards such as HIPAA, GDPR, or PCI-DSS often require detailed audit trails, comprehensive access logs, and advanced encryption methods. Achieving these with S3 demands significant time and resource investment. While S3 may have impressive durability numbers, its security capabilities are limited when it comes to comprehensive backup needs.\nVersioning and deletion: a double-edged sword # S3 versioning is often viewed as a safety net for backups. In practice, however, it can work against you. Versioning allows you to retrieve older copies of your objects, but it also leaves you vulnerable if all versions are accidentally or maliciously deleted. MFA delete is intended to offer extra protection, but it can make even intentional deletions more complicated. Object Lock might seem like a solution for compliance, but it also means you can never completely remove the data if necessary.\nThe features that S3 provides to help with data recovery can sometimes hinder recovery efforts in a crisis. The backup world requires both durability and flexibility, along with rapid recovery capabilities. S3’s design falls short in this regard, often leaving you with a solution that works under ideal conditions but may fail when you need it most.\nComparing S3 with dedicated backup solutions # When it comes to data protection, you have two choices: force S3 into a role it was not designed for or use tools built specifically for backup. Here is a comparison of these options.\nTailored backup features versus S3\u0026rsquo;s generalist approach # Dedicated backup solutions are engineered for backup and recovery. They offer features such as incremental and differential backups, automated snapshotting, and rapid point-in-time restores. These systems are built with the assumption that mistakes will happen, whether due to human error or unforeseen issues, and they are designed to minimize downtime and data loss.\nS3, by contrast, is a general-purpose storage service. It reliably stores data but does not handle the nuances of backup cycles, retention policies, or quick recovery times. For instance, a dedicated backup system can restore a single file from a specific moment in time, while with S3, you may have to manually search through multiple versions. When disaster strikes, it is not as simple as instructing S3 to \u0026ldquo;roll back\u0026rdquo; and expect everything to be restored instantly.\nCost, complexity, and management overhead # At first glance, S3 may seem like a cheaper option due to its pay-as-you-go pricing. However, when you factor in the additional software, manual processes, and ongoing monitoring needed to make S3 work as a backup, the costs can quickly add up. Dedicated backup solutions come with integrated management interfaces, reporting tools, and automated recovery procedures that simplify operations and reduce the risk of human error.\nThe management overhead is not only a financial concern; it is also a matter of time and effort. Keeping track of encryption keys, version histories, and access policies in S3 can become a logistical challenge. In contrast, a dedicated backup system is designed to integrate seamlessly with your workflows, allowing you to focus on ensuring your data is restorable when you need it most.\nBest practices for a rock-solid backup strategy # No one is immune to mistakes. Fat-fingered deletions, configuration errors, and unforeseen mishaps are all part of managing data. That is why you need a backup strategy that is as layered as your overall security measures. Here are some best practices for building a robust backup system.\nEmbracing a multi-layered backup approach # Relying on a single backup method is a recipe for disaster. Instead, adopt a multi-layered strategy. S3 is excellent for storing massive amounts of data economically, but for critical data, you need multiple copies in different locations. Use local backups for rapid recovery, integrate cloud-native backup tools for continuous data protection, and consider offsite backups with other providers for additional security.\nSome organizations use S3 for archival purposes while relying on dedicated backup appliances or software for daily snapshots and rapid restores. This redundancy ensures that if one backup fails, another layer is ready to take over.\nLeveraging the right tools for the job # Not all backup tools are created equal. Choose systems that offer automated testing, granular recovery options, and seamless integration with your existing infrastructure. Whether you opt for a commercial backup solution or an open-source alternative, make sure it supports features like incremental backups, easy-to-use dashboards, and robust encryption. The goal is to create a system where every piece of data can be tracked, restored, and verified without having to perform complex maneuvers in a crisis.\nLessons from real-world case studies # Real-world experiences provide valuable lessons. Many organizations have discovered, often the hard way, that relying solely on S3 can lead to prolonged downtime and painful recovery processes. For example, one mid-sized firm experienced a major data loss due to accidental mass deletion. They mitigated the impact by integrating S3 with a dedicated backup solution, which not only reduced recovery times but also improved overall data governance. Regular testing of backup processes can reveal weaknesses before a real crisis hits and ensure that when mistakes occur, your data is safe and recoverable.\nQuick takeaways # S3 is excellent for scalable, high-availability object storage, but it is not a backup solution. S3\u0026rsquo;s eventual consistency and versioning can create significant recovery challenges. Security configurations in S3 require constant vigilance to protect sensitive backup data. Dedicated backup solutions offer granular recovery, automated testing, and true point-in-time restores. A multi-layered backup strategy that includes local, cloud, and offsite backups minimizes risk. Conclusion # In summary, while Amazon S3 is a robust platform for storing large amounts of data with impressive durability, it is not engineered to serve as a comprehensive backup solution. S3\u0026rsquo;s architecture emphasizes high availability and cost efficiency, not the nuanced demands of rapid recovery, granular version control, or robust security in backup scenarios. Relying solely on S3 for backup is similar to using a reliable delivery truck as an armored vault; it transports your data effectively but is not designed to handle every contingency.\nA thoughtful backup strategy requires multiple layers: local backups for speed, cloud backups for redundancy, and offsite solutions for additional security. Integrating dedicated backup tools alongside S3 can help prevent the issues of accidental deletions, malicious actions, and misconfigurations that could lead to catastrophic data loss. Investing in a comprehensive backup solution is essential because when it comes to protecting critical data, durability alone is not enough.\nFAQs # 1. Why is S3 not enough as a standalone backup solution?\nS3 is designed for high-availability object storage rather than the nuanced requirements of backups such as point-in-time recovery, incremental backups, or granular restoration. Its eventual consistency model may delay recovery, making it unsuitable for critical backup needs.\n2. Can S3\u0026rsquo;s versioning be used effectively for backups?\nWhile S3 versioning can help recover older copies of objects, it is not foolproof. Accidental or malicious deletion of all versions can leave you without a fallback, and features like MFA delete complicate the process further.\n3. How do dedicated backup solutions compare to using S3 alone?\nDedicated backup solutions offer automated snapshotting, incremental backups, and rapid recovery options specifically tailored for disaster scenarios. They also include robust encryption and management features that make data restoration simpler and more reliable.\n4. What is a multi-layered backup strategy?\nA multi-layered backup strategy combines various methods—local backups for fast recovery, cloud-based backups for redundancy, and offsite solutions for disaster resilience—to ensure that if one layer fails, other copies remain available.\n5. How can I integrate S3 with a dedicated backup solution?\nMany modern backup platforms provide seamless integration with S3. These solutions use S3 for cost-effective archival storage while managing real-time backups and rapid restores through specialized software. This hybrid approach leverages the strengths of S3 without exposing you to its limitations.\n","date":"10 February 2025","externalUrl":null,"permalink":"/insights/s3-is-not-a-backup---why-you-need-a-real-backup-strategy/","section":"Ready to get some insights?","summary":"\u003cp\u003eLet us get straight to the point: Amazon S3 is a phenomenal service for scalable, reliable object storage but it is not a backup solution. Sure, S3 boasts rock-solid durability and cost efficiency, but relying on it alone for backups is like trying to cover your bases with duct tape. In today’s world, where a single misclick can spell disaster, a thoughtful, multi-layered backup strategy is not just nice to have; it is absolutely essential.\u003c/p\u003e","title":"S3 is not a backup: why you need a real backup strategy","type":"insights"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/categories/technology/","section":"Categories","summary":"","title":"Technology","type":"categories"},{"content":"In the realm of data protection, backup and replication are two fundamental strategies employed to safeguard information. While they share the common goal of data preservation, they operate on distinct principles and serve different purposes. Understanding these differences is crucial for developing a robust data protection strategy.\nUnderstanding backup and replication # What is data backup? # Data backup involves creating copies of data at specific points in time that can be restored in the event of data loss or corruption. These backups are typically stored separately from the original data, often offsite or in the cloud, to protect against disasters. Backups can be full, incremental, or differential depending on the organization\u0026rsquo;s needs.\nWhat is data replication? # Data replication entails creating and maintaining duplicate copies of data across multiple locations or systems. This process ensures that data is continuously available and accessible even if one system fails. Replication can be synchronous, where data is copied in real time, or asynchronous, where data is copied at scheduled intervals.\nKey differences between backup and replication # Purpose and objectives # Backup: Provides restore points for recovering data after loss or corruption. Replication: Ensures continuous availability by maintaining real-time copies of data. Data consistency and recovery # Backup: Lets you restore data to a specific point in time, making it ideal for recovering from accidental deletions or corruption. Replication: Keeps copies consistent with the original but does not offer historical versions for recovery. Impact on performance # Backup: Typically scheduled during off-peak hours to minimize impact on performance. Replication: Continuously updates data, which can affect system performance, especially in high-volume environments. Use cases # Backup: Best for long-term data retention, compliance, and protection against accidental data loss. Replication: Ideal for mission-critical systems requiring high availability and rapid recovery. Complementary roles in data protection # While backup and replication serve different purposes, they are complementary components of a comprehensive data protection strategy. Combining both ensures that data is both readily available and protected against various threats.\nExample: iCloud and the difference between backup and replication # Consider the example of iCloud to illustrate the difference between replication and backup. iCloud replicates the photos on your phone to the cloud, but it does not create traditional backups of them.\nScenario 1: Accidental deletion by a child # Imagine you leave your phone unattended and your child begins to explore it. In their curiosity, they accidentally delete some important photos. Since iCloud replicates the photos in real time, these deletions are immediately reflected in your iCloud storage. In this case, the deletion is instantly replicated to iCloud, and because iCloud does not keep historical versions, the deleted photos are permanently lost. If you\u0026rsquo;re lucky, you might be able to find these photos in your trash, in the Recently Deleted section on iCloud. However, this option is only available for a limited time. If the deletion occurred several months ago, the photos may have already been permanently removed from your trash, making recovery very difficult or even impossible. In that case, if you don\u0026rsquo;t have a backup elsewhere, such as an external hard drive or another backup service, you may lose those precious memories for good.\nScenario 2: Data corruption from a third-party app # Suppose you download a third-party app to edit your photos. Although the app appears safe, it malfunctions or contains a bug that causes some of your photos to become corrupted. After syncing with iCloud, the corrupted photos are also replicated to the cloud. Replication synchronizes the corrupted data, and since iCloud does not allow you to restore previous versions of the photos, the damage is irreversible. With a backup strategy in place, you could easily restore the original, uncorrupted photos.\nScenario 3: Data loss after a breakup # Imagine a personal scenario where, after a breakup, your ex-partner who still has access to your iCloud account or your phone decides to delete all of your shared photos. Because iCloud replicates changes made on the phone, any deletions are immediately reflected on the cloud. iCloud does not provide a way to roll back the deletions, and the photos are permanently gone. However, if you had a separate backup, such as a hard drive backup or another cloud service, you could have recovered those precious memories even after this unexpected event.\nConclusion # In the world of data protection, backup remains an indispensable component. Even the most robust replication strategy cannot replace the need for backups. Replication keeps data available in real time but fails to protect against scenarios such as human error, corruption, or malicious actions. Such errors are simply mirrored across your replicated systems.\nThink of replication as a safety net that ensures continuous data access. Without backups, however, this net does not catch your mistakes. Backups serve as a safety vault that allows you to recover data to a specific point in time and prevents the irreversible loss of critical information. Relying solely on replication, without the safeguard of backups, leaves data vulnerable to irreparable damage whether due to accidental deletions, software bugs, or cyber threats.\nFor organizations and individuals alike, it is crucial to understand that replication cannot replace backups. A proper data protection strategy requires both to truly secure valuable information.\nQuick takeaways # Backup creates copies of data at specific points in time for recovery purposes. Replication maintains real-time copies of data across multiple locations for high availability. Backups are cost-effective and suitable for long-term data retention. Replication requires significant infrastructure investment and can impact system performance. Combining both strategies enhances data protection and recovery capabilities. FAQs # Can replication replace backups?\nNo, replication ensures data availability but does not provide historical recovery points like backups do.\nHow does replication affect system performance?\nContinuous data replication can consume system resources and may impact performance, especially in high-volume environments.\nIs replication more expensive than backup?\nYes, replication typically requires more infrastructure and storage, making it more costly than traditional backup solutions.\nCan replication be used for disaster recovery?\nYes, replication is a key component of disaster recovery plans, ensuring data availability in case of system failures.\nHow often should backups and replications be performed?\nBackups should be scheduled based on data change frequency and compliance requirements, while replication frequency depends on the criticality of the data and the organization\u0026rsquo;s recovery objectives.\n","date":"10 February 2025","externalUrl":null,"permalink":"/insights/why-replication-is-not-backup/","section":"Ready to get some insights?","summary":"\u003cp\u003eIn the realm of data protection, \u003cstrong\u003ebackup\u003c/strong\u003e and \u003cstrong\u003ereplication\u003c/strong\u003e are two fundamental strategies employed to safeguard information. While they share the common goal of data preservation, they operate on distinct principles and serve different purposes. Understanding these differences is crucial for developing a robust data protection strategy.\u003c/p\u003e","title":"Why replication is not backup","type":"insights"},{"content":"Data loss can happen in many ways: whether due to accidental deletion, cyberattacks, hardware failure, or even a catastrophic event like a data center fire. To protect against these risks, IT professionals have long relied on the 3-2-1 backup rule, a fundamental strategy for ensuring data resilience.\nThis article breaks down what the 3-2-1 backup rule is, why it is critical, and why replication or single-cloud backups are not enough. We also explore the types of threats it mitigates, from hacker intrusions to storage provider failures, and how to implement it effectively with proper offline or air-gapped backups.\nWhat is the 3-2-1 backup rule? # The 3-2-1 backup rule is a best-practice guideline for data redundancy and disaster recovery. It ensures that organizations maintain sufficient copies of their data to minimize the risk of total data loss.\nThe core principle of 3-2-1 # The rule dictates that you should:\nKeep at least 3 copies of your data (one primary plus two backups). Store backups on at least 2 different types of media (for example, a local disk and cloud storage, or a local NAS and tape). Ensure 1 backup copy is off-site (in a different location or cloud service, ideally offline or air-gapped). This approach guarantees that even if one or two copies are lost, a third copy remains accessible for recovery.\nExample of a proper 3-2-1 backup implementation # Let us say you run a critical business application storing important customer data:\nPrimary Copy – The data lives on your production server (for example, an on-premise storage system or S3 storage). Secondary Copy – A backup is stored on a separate NAS, another cloud storage, or a different disk-based system. Tertiary Copy (Off-Site and Air-Gapped) – A cloud backup stored in AWS Glacier with a multi-day deletion delay, or a tape backup stored in a secure facility with robotic retrieval. This ensures that even if your main server fails, your local backup is corrupted, or your cloud provider is compromised, an air-gapped copy remains protected.\nWhy is it important to implement the 3-2-1 backup rule? # A single backup is never enough because data loss comes from many unpredictable sources, including:\nHuman errors – Accidental file deletions or unintended data overwrites. Hardware failures – Disk crashes, server failures, or RAID corruption. Cyber threats – Ransomware, malware, or hacker intrusions. Administrative mistakes – Accidental database deletions or misconfigurations. Cloud service failures – Unexpected outages or accidental deletions by providers. Physical disasters – Fires, floods, earthquakes, or power failures. Rogue admins – Malicious insiders deleting backups or modifying retention policies. A multi-layered backup strategy like 3-2-1 ensures that even if one or two of these failures occur, you still have a recoverable copy of your data.\nWhy replication is not a backup # One common misconception is that replication can replace backup. This is not true. While replication is useful for availability, it does not protect against data corruption, accidental deletions, or cyberattacks.\nKey differences between backup and replication # Aspect Backup Replication Purpose Disaster recovery High availability Retention Keeps historical versions Only keeps the latest version Data corruption protection Older copies remain untouched Corruption is replicated immediately Protection from human error Can restore from a clean backup Deletes or mistakes are instantly mirrored Protection from ransomware Can recover from an old snapshot Ransomware spreads to replicated copies Why replication fails as a backup strategy # Imagine an admin accidentally deletes a critical database. If your system only uses replication:\nReplication immediately mirrors the deletion across all systems so that the data is lost everywhere. There is no historical backup to restore from, meaning you cannot go back in time. If ransomware encrypts files, the encrypted data is also replicated immediately. In contrast, a proper backup solution with versioning allows recovery from an earlier, uncorrupted state.\nRead more about this topic: Why replication is not backup?\nWhy a backup in the same cloud account is not enough # Cloud services like AWS, Google Cloud, and Azure offer native snapshots and backups. While these options seem convenient, relying solely on one cloud provider can be a serious mistake.\nThe risks of same-cloud backups # Hacker intrusions and ransomware If an attacker gains access to your cloud account, they can delete all snapshots and backups. Many cloud providers allow instant deletion of backups, making recovery difficult. Solution: Store off-site backups with multi-day deletion delays, such as AWS Glacier Vault Lock. Storage provider failures Storage provider-side failures can happen. For example, Amazon S3 experienced data loss incidents due to misconfigurations, and Google Cloud once accidentally deleted customer backups because of an internal process error. Solution: Store at least one copy on a separate cloud provider or on-premise tape storage. Account termination risks If your cloud provider suspends or terminates your account, you could lose access to both production and backup data stored in that same cloud. Solution: Store an additional copy in a different cloud provider or physical tape archive. Conclusion # The 3-2-1 backup rule remains a simple yet powerful strategy to protect against a wide range of data loss scenarios.\nReplication is not backup because it does not protect against accidental deletions, corruption, or ransomware. A single cloud backup is not enough since provider failures, rogue admins, or account terminations could lead to permanent data loss. Offline or air-gapped backups are critical. Tape storage or AWS Glacier with deletion locks ensures that backups cannot be easily deleted. By keeping multiple copies on different media, and one truly protected off-site, you ensure resilience against both human and technical failures. No matter the size of your organization, implementing a proper 3-2-1 backup strategy is essential to safeguard data against disaster.\nQuick takeaways # 3-2-1 backup rule fundamentals:\nMaintain at least three copies of your data on two different types of media, with one copy stored off-site or in an air-gapped environment. Backup vs. replication:\nReplication ensures high availability but mirrors errors and corruption immediately, making it insufficient as a standalone backup strategy. Comprehensive threat protection:\nA robust backup strategy defends against human error, hardware failures, cyberattacks (including ransomware), and physical disasters. Limitations of cloud-only backups:\nRelying solely on one cloud provider can expose your data to risks such as security breaches, misconfigurations, or account terminations. Importance of offline/air-gapped backups:\nOffline or air-gapped backups (for example, tape storage or AWS Glacier with deletion locks) are critical to prevent accidental or malicious data deletions. Ensuring data resilience:\nA multi-layered backup approach, as outlined by the 3-2-1 rule, guarantees that even if one or more copies are lost, your data remains recoverable. ","date":"10 February 2025","externalUrl":null,"permalink":"/insights/the-3-2-1-backup-rule---a-proven-strategy-for-data-protection/","section":"Ready to get some insights?","summary":"\u003cp\u003eData loss can happen in many ways: whether due to accidental deletion, cyberattacks, hardware failure, or even a catastrophic event like a \u003cstrong\u003edata center fire\u003c/strong\u003e. To protect against these risks, IT professionals have long relied on the \u003cstrong\u003e3-2-1 backup rule\u003c/strong\u003e, a fundamental strategy for ensuring data resilience.\u003c/p\u003e","title":"The 3-2-1 backup rule: A proven strategy for data protection","type":"insights"},{"content":"","date":"18 December 2024","externalUrl":null,"permalink":"/","section":"Home","summary":"","title":"Home","type":"page"},{"content":" Publisher # Plakar Simplified Joint-Stock Company (SAS) with a capital of 1,000 euros SIREN: 933 509 754 RCS Paris Head office: 149 avenue du Maine, 75014 Paris, France EU VAT number: FR XX 933509754 Host: KANDBAZ SAS, 1 rue de Stockholm, 75008 Paris, France\nContact Us # Mail: Plakar SAS, 149 avenue du Maine, 75014 Paris, France Technical Support: help@plakar.io\nAbuse # To report illegal content or if you are the victim of fraudulent use of our services, please contact us at: help@plakar.io\nIntellectual Property # This website and all its content (including data, information, photos, logos, and trademarks) are the exclusive property of Plakar SAS or its partners. Any reproduction, representation, translation, adaptation, or citation, whether partial or complete, regardless of the process or medium, is strictly prohibited except as provided by law or expressly authorized by their owner. Non-contractual photos.\nPersonal Data # You can visit our website without having to disclose your identity or provide personal information. However, we may request information from you to process an order, identify a technical support request, correspond with you, provide a subscription, or submit a job application.\n","date":"18 December 2024","externalUrl":null,"permalink":"/legal-notice/","section":"Home","summary":"Publisher # Plakar Simplified Joint-Stock Company (SAS) with a capital of 1,000 euros SIREN: 933 509 754 RCS Paris Head office: 149 avenue du Maine, 75014 Paris, France EU VAT number: FR XX 933509754 Host: KANDBAZ SAS, 1 rue de Stockholm, 75008 Paris, France","title":"Legal Notice","type":"page"},{"content":"Hello teams\n","date":"18 December 2024","externalUrl":null,"permalink":"/team/team/","section":"Teams","summary":"Hello teams","title":"Team","type":"team"},{"content":"","date":"18 December 2024","externalUrl":null,"permalink":"/team/","section":"Teams","summary":"","title":"Teams","type":"team"},{"content":"","date":"13 November 2024","externalUrl":null,"permalink":"/tags/backups/","section":"Tags","summary":"","title":"Backups","type":"tags"},{"content":" Read about Plakar latest development update. ","date":"13 November 2024","externalUrl":null,"permalink":"/community-news/","section":"Plakar community updates","summary":"Read about Plakar latest development update.","title":"Plakar community updates","type":"community-news"},{"content":"Hello Plakar pals and curious coders! Plakup here, bringing you the freshest acorns of updates from the Plakar hackroom! We’ve had a whirlwind of activity, new faces joining the team, and some clever improvements from Gilles, Julien, and the whole gang. So grab a coffee (or a handful of sunflower seeds, if you’re like me), and let’s jump into the latest!\n1. Welcome Julien Castets! # First things first, let’s give a warm Plakar welcome to Julien \u0026ldquo;niluje\u0026rdquo; Castets, who joined us on November 11! 🎉 Julien has already jumped in with creative ideas and code contributions (not to mention his GitHub Action adventures). We’re thrilled to have him onboard and excited to see the magic he’ll bring to Plakar’s UI and more!\n2. File Testing and the Search for a Pathological Corpus 🗂️ # The team has been diving into file testing strategies, with discussions around a “corpus” to push Plakar’s boundaries. The idea? A hybrid set of test files that cover all sorts of types: from small files to hefty images, randomized data, and even edge cases like empty files, /dev/zero, and /dev/random files, and cyclic symlinks. By testing these edge cases, the team ensures that Plakar handles any file you throw its way! 🌞\n3. Fixing Permissions with Precision: chmod Restoration Updates 🔧 # One of Gilles’s top priorities this week was addressing a restore permissions issue. After a bit of restructuring, the restore now handles permissions more gracefully by making directory creation (with a secure 0700 mode) a standalone step. Once child files are processed, it finalizes permissions with SetPermissions, ensuring everything restores exactly as it should—hats off, Gilles!\n4. UI Revamp and Automation with GitHub Actions # Julien didn’t waste any time getting to work on enhancing Plakar’s UI workflow. He’s been wrestling with GitHub Actions, setting up a process to automate builds, updates, and even PR creation between plakar-ui and the main Plakar repository. After some experimentation (and a few “aha!” moments), Julien proposed a more streamlined approach: a manual workflow in Plakar that lets us specify branches for seamless UI updates with a single click. The ambition and energy Julien’s brought to Plakar’s UI are nothing short of inspiring! 🚀\nExperimental UI Structure # Julien also suggested consolidating some of the UI folders, like ui/v2, to simplify the setup. Although this change was closed to keep options open for future experiments, it shows the team’s forward-thinking approach as they plan for long-term UI flexibility.\n5. Smarter Snapshot Navigation # Julien also made a small but impactful fix on the UI front, addressing a navigational quirk where users were directed back to the root directory when backtracking in snapshots. Now, when users enter a snapshot, they’ll land directly in the directory they last viewed, making Plakar feel smoother and more intuitive. With Julien’s contributions, the UI is already shaping up beautifully.\n6. Path Normalization for Backup \u0026amp; Restore 🛠️ # To tackle path inconsistencies across case-sensitive and insensitive filesystems, Gilles added a nifty enhancement to store the current working directory (CWD) as normalized context. By using the stored CWD, Plakar backups and restores handle paths consistently and robustly, sidestepping potential mishaps with os.Getwd().\n7. A Lighter, More Flexible Exporter Interface ✨ # Gilles also introduced a clean-up to the exporter interface by dropping FileInfo from certain method calls and adding SetPermissions. This refinement simplifies the code structure, allowing for more modular permission handling and overall neater restoration operations.\n8. Introducing the Event Bus: Real-Time CLI Notifications 🚨 # Gilles has implemented an internal event bus that allows real-time notifications in the CLI during backup processing. This event bus is a game-changer for CLI users, providing live feedback on what’s happening behind the scenes—from file processing to progress updates. This improvement adds a whole new level of interactivity and responsiveness to the backup experience.\nClosing Thoughts: Teamwork Makes the Magic # This week brought not just improvements to code but lively banter, fresh ideas, and an undeniable sense of camaraderie among the team. Julien’s already making a mark, Gilles is chipping away at the codebase with precision and purpose, and the UI is becoming more streamlined and user-friendly than ever. Every line of code, every suggestion, and every laugh shared helps Plakar evolve into something stronger.\nUntil next time, Plakar enthusiasts! Keep those snapshots sharp, code clean, and permissions perfect! 🐿️\n— Plakup, your friendly Plakar chipmunk\n","date":"13 November 2024","externalUrl":null,"permalink":"/community-news/2024-11_01/","section":"Plakar community updates","summary":"Hello Plakar pals and curious coders!","title":"Plakar Hackroom Highlights: New Faces, Fresh Fixes, and UI Upgrades 🐿️","type":"community-news"},{"content":"Hello Plakar pals and curious coders! Plakup here, ready to share all the exciting updates from the past week. Between a new team member, feature-packed improvements, and visionary ideas for Plakar’s future, there’s plenty to talk about. So grab a coffee (or your favorite acorn snack), and let’s dive in!\n1. Team Updates: Welcome Omar ! # We’re thrilled to announce that Omar Polo (@op) joined the team last week! 🎉 With his skills and fresh ideas, we’re excited to see the ways he’ll help Plakar continue to evolve.\n2. Reviving Features and Streamlining Code 🛠️ # FUSE is Back! # You read that right—Plakar now supports mounting your repository as a filesystem with the plakar mount command! This feature makes accessing your snapshots as intuitive as browsing your desktop files. FUSE integration is back and better than ever.\nSnapshot Signing # Your data has never been safer! Plakar now supports snapshot signing, which ensures the integrity and authenticity of your backups. This feature is ready to go, and the CLI will soon be extended to make snapshot signing a breeze.\nPer-Directory Summaries and Error Tracking # Debugging backups just got easier. Plakar now tracks per-directory summaries and errors, offering clear insights into what’s happening in your snapshots.\nS3 Importer Revamp # The S3 importer has been updated to match the new importer interface, following the recent repository refactor. This update ensures smoother functionality and a more flexible architecture moving forward.\nRepository and State Refactor # Gilles undertook a significant effort to simplify the core repository and snapshot state layers. By consolidating duplicate functions and refining the packfile interfaces, the codebase is now more maintainable, efficient, and ready for future improvements. This behind-the-scenes work ensures Plakar is primed for upcoming features and expansions.\n3. UI Enhancements: Downloads and More! 📥 # The Plakar UI is evolving fast! You can now download files directly from the UI with a single click. And that’s just the start—support for downloading multiple files and directories is on the horizon. Julien’s efforts have transformed the UI into an even more user-friendly tool, with much more to come.\n4. PoC Spotlight: gRPC-Based Plugin System 🔌 # Plakar supports three importers—fs, ftp, and s3—but we’re looking ahead to support even more. Julien has been prototyping a gRPC-based plugin system. Here’s the vision:\nThe Plugin Architecture # Standalone Plugins: Importers now run as separate binaries communicating with Plakar via gRPC. Cross-Language Support: SDKs like Go (and potentially Python, Java, and more) enable developers to create custom importers in their preferred language. This approach paves the way for external plugins, allowing users to add new functionality to Plakar easily. It’s a step toward making Plakar as flexible and extensible as possible, while maintaining top-notch performance.\n5. Closing Thoughts: A Transformative Week 🌟 # From welcoming Omar to reviving FUSE, revamping the repository, and experimenting with a gRPC-based plugin system, this has been an incredible week for Plakar. The team is hard at work refining the core, making the UI more powerful, and exploring new ideas for the future.\nUntil next time, keep your snapshots secure, your plugins polished, and your acorns plentiful! 🐿️\n— Plakup, your friendly Plakar chipmunk\n","date":"13 November 2024","externalUrl":null,"permalink":"/community-news/2024-11_02/","section":"Plakar community updates","summary":"Hello Plakar pals and curious coders!","title":"Plakar Hackroom Highlights: New Features, Fresh Faces, and Big Ideas 🐿️","type":"community-news"},{"content":"Hello, Plakar enthusiasts and curious readers alike! Plakup here—Plakar\u0026rsquo;s very own chatty chipmunk correspondent. 🐿️ It\u0026rsquo;s been an exciting and productive few days at Plakar Korp, with lots of brainstorming, coding, and passionate exchanges between the community members. We thought we\u0026rsquo;d take a moment to highlight all the cool stuff that\u0026rsquo;s been happening, so grab a cup of coffee (or a handful of acorns, if you\u0026rsquo;re like me), and let’s dive in!\n1. Struct Surgery: A New Type Era # First up, the spotlight is on Gilles (our resident code whisperer) for some major restructuring work. Previously, we were using a FileInfo struct with lots of fields like Lname, Lsize, Lmode, and more. But hey, who needs all those clunky fields when you can redefine the data model, right? Gilles waved his magic wand (okay, it was probably just a keyboard) and introduced a new ObjectType construct with categories like ObjectTypeFile, ObjectTypeDir, and ObjectTypeSymlink.\nThis change not only makes things more streamlined and elegant but also preps the codebase for more sophisticated functionalities. A round of applause, everyone! 🎉\n2. RIP Agent Layer \u0026amp; Config Layer: Gone but Not Forgotten # Next, we had a bittersweet moment—Gilles decided to pull the plug on the agent and config layers. Don’t worry, they were just prototypes. Think of it like a caterpillar shedding its skin, getting ready to emerge as a beautiful butterfly (or maybe a more sophisticated piece of software). 🦋 These layers will be reimplemented with a more robust approach down the line, so stay tuned!\n3. Enviable Environment Variables # Security and flexibility are paramount in Plakar’s world. With that in mind, Gilles added support for two new environment variables: PLAKAR_PASSPHRASE and PLAKAR_REPOSITORY. Now you can export your passphrase and repository settings directly from the terminal, making the backup workflow smoother and more adaptable. No more fumbling around with long command lines! ✨\nHere’s a quick recap:\n$ export PLAKAR_PASSPHRASE=totototo $ plakar create $ plakar push $ plakar ls 2024-10-10T23:09:34Z 6d7a9690 4.0 GB 9s /Users/gilles/Wip/github.com/PlakarLabs $ And voilà! Snapshots are safely created and listed. 🗃️\n4. Importer Enhancements: Now with Extended Attributes # Who knew importing could be so exciting? Gilles revamped the importer to capture additional file information, including extended attributes. Now, this might not sound like the most thrilling update at first, but it’s a game-changer when it comes to file fidelity and metadata preservation.\n5. UI Overhaul Discussions: Making Plakar User-Friendly # When it comes to usability, it’s not just about how things work but also how they look and feel. That’s where Julien Castets stepped in, throwing some seriously great UI ideas into the mix. We’ve been brainstorming different views for snapshots:\nSnapshots View: Lists snapshots in chronological order and shows file lists by default. File Kind/Mime-type View: Displays files based on their type (e.g., images, documents). Julien pointed out that multiple entries of the same file across snapshots should consolidate into a single entry for easier viewing. Great point, Julien! 👍\nGlobal Search: To quickly locate files across all snapshots with just one search bar. Simplicity at its finest. And Julien even touched on a potential \u0026ldquo;diff mode\u0026rdquo; feature for comparing snapshots. A little fine-tuning is needed here, but the team’s onto something big.\n6. Diffs \u0026amp; Comparisons: The Path Forward # Snapshot comparisons are a cornerstone of Plakar’s backup strategy. Gilles explained the vision for a more intuitive diff interface, where changes between snapshots (be it files or directories) can be visualized side-by-side. Here’s what he has in mind:\nTwo-Column View: Think of it as a split screen where changes are highlighted—what’s new, what’s missing, and what’s been modified. Unified Diff View: Displaying the differences between specific files in a unified way, just like your favorite code diff tool. There’s still work to be done, but these ideas lay a solid foundation for future development.\n7. Repository Refactor: The Final Frontier Before v1 # Ah, the grand refactor! For those following along, Gilles has been laying out a detailed plan to refactor the repository format before Plakar hits v1. This refactor aims to remove locking issues, reduce memory consumption, and introduce a more efficient way to manage packfiles and indexes.\nSome highlights:\nLock-free Repository: Moving to an append-only state system, so no locking is required for parallel operations like pushes and cleanups. Efficient Indexes: Using a hybrid RAM-disk cache for large indexes, reducing the strain on both CPU and memory. Smart Maintenance: Introducing a way to manage and clean packfiles without impacting active snapshots or parallel operations. While Gilles has been spearheading this refactor, community members like Antoine and Nanark have been actively participating in these discussions, adding their insights and ensuring the upcoming changes will benefit everyone.\nHere’s a snippet from one of Gilles’ updates:\n\u0026ldquo;The refactor is going to require some work, but it’s far less invasive than I first assumed. The first thing I need to change is the handling of indexes. Right now, the indexes contain the entire mapping. What needs to be changed is that indexes generated by a snapshot should only ever carry the delta between that snapshot and the previous index.\u0026rdquo;\nClosing Thoughts: Progress is a Team Sport # It’s not just about the code. It’s about the conversations, the debates, and the shared vision. Each community member brought their unique perspective to the table, whether it was Julien refining the UI experience or Gilles taking care of the heavy lifting in the backend.\nAs Plakar continues to evolve, expect more innovation, more polish, and (hopefully) fewer sleepless nights for our developers. Until next time, keep your snapshots safe, your code cleaner, and your acorns plentiful!\n— Plakup, your chirpy Plakar companion 🐿️\n","date":"12 October 2024","externalUrl":null,"permalink":"/community-news/2024-10_01/","section":"Plakar community updates","summary":"Hello, Plakar enthusiasts and curious readers alike!","title":"Plakup's Chronicles: A Journey Through Recent Plakar Developments","type":"community-news"},{"content":"Back in August 2019, a project called Plakar began to take shape. Initially an experiment, it was developed by Gilles Chehade as a personal response to dissatisfaction with existing backup solutions. While many backup tools promise reliability and ease, they often come with limitations that become apparent at scale. Plakar aimed to address these issues head-on, focusing on simplicity, speed, and flexibility. Let\u0026rsquo;s dive into what makes Plakar unique and its evolution over time.\nWhy Plakar? # Plakar was conceived out of a desire for a more lightweight, flexible, and scalable backup solution. Existing backup systems either felt overly complex or couldn’t handle large datasets efficiently without consuming tons of resources. Most required expensive servers to run or were riddled with features that were unnecessary for everyday backups.\nPlakar’s core goal from the outset was simple: build a fast and efficient backup tool that anyone can run, even on modest hardware. It was born out of frustration but quickly grew into a fully-fledged project that promised a better way to think about backups.\nThe Design Philosophy # At its core, Plakar took inspiration from Git’s repository model. Just like Git tracks changes in code repositories over time, Plakar tracks snapshots of a filesystem. However, it avoids some of the bloat and complexity associated with Git for backup use cases.\nFilesystem Snapshot Model: Plakar takes snapshots of the file system, storing only the differences between snapshots (much like a version control system). This allows for fast incremental backups without having to re-copy unchanged data. Data Deduplication: One of the standout features in Plakar’s design is deduplication, which means that identical data across multiple snapshots is only stored once. This is crucial for large datasets, where redundancy can quickly bloat storage. Designed for Modesty: Unlike heavyweight solutions that demand significant resources, Plakar was built to run on modest hardware. Whether you’re backing up to a small NAS or a low-powered server, Plakar’s design ensures that the system is both efficient and lean. Evolution Over Time # What began as an experiment quickly matured into a more feature-complete solution. Gilles emphasized that Plakar is not a one-size-fits-all solution—it doesn’t try to be a full-featured, all-encompassing backup system. Instead, Plakar focuses on doing one thing well: efficiently managing backups on a local or remote system.\nAt the heart of Plakar’s success is its incremental development process, where each new feature added is carefully considered for its real-world utility. From deduplication and delta snapshotting to its Git-like model for managing files, Plakar’s progression is marked by a constant desire to simplify and improve the backup process.\nThe Road Ahead # While Plakar has come a long way since its early days, there’s always more work to be done. Future plans for Plakar include improved repository handling and support for additional storage backends, making it even more flexible.\nThe core philosophy, however, remains the same: keep it simple, keep it fast, and keep it lightweight. Whether you’re running backups on a massive data center or a small home server, Plakar’s minimalist design ensures that it can scale and adapt to your needs without unnecessary complexity.\nIn short, Plakar has grown from an experimental tool into a reliable and efficient backup solution for those who value simplicity, performance, and flexibility. Whether you’re protecting your personal data or managing large-scale backups, Plakar is designed to make that process as smooth and resource-friendly as possible.\nStay tuned for more updates as Plakar continues to evolve!\n","date":"20 July 2024","externalUrl":null,"permalink":"/community-news/2024-07_02/","section":"Plakar community updates","summary":"Back in August 2019, a project called Plakar began to take shape.","title":"Plakup's Chronicles: A Glimpse into its Early Days and Design Philosophy","type":"community-news"},{"content":"Hello, world! 🌍\nI’m Plakup, your friendly neighborhood chipmunk correspondent from Plakar Korp, and I’m here to keep you in the loop on all things Plakar—the fast, efficient backup solution you never knew you needed.\nWhat’s Plakar, you ask? Well, imagine if your favorite backup system had the elegance of Git, the simplicity of a good cup of coffee, and the efficiency of a chipmunk storing acorns for winter. Plakar is all about keeping your data safe without breaking a sweat, whether you\u0026rsquo;re managing small home backups or handling large-scale systems.\nWhat Is This Blog About? # This blog is your one-stop hub for all things Plakar. I’ll be sharing updates, tips, deep dives, and behind-the-scenes insights into the latest developments in Plakar\u0026rsquo;s world. Whether it\u0026rsquo;s the latest feature releases, technical how-tos, or exciting community-driven discussions, this is where you’ll find it all.\nHere’s what you can expect from our future posts:\nDevelopment Updates: Get the lowdown on what’s new, what’s improved, and what’s coming next for Plakar. Feature Spotlights: Dive into specific features, how they work, and how they can help you streamline your backup workflows. Community Contributions: Learn how Plakar users are making an impact and discover new ways to make the most of the tool. Technical Deep Dives: For the geeks out there (you know who you are 🧑‍💻), I’ll be dissecting some of the more complex aspects of Plakar’s architecture. Who Am I? # I’m Plakup, your resident chipmunk correspondent! 🐿️ My job is to turn the techie stuff into bite-sized, digestible pieces that everyone can understand and enjoy. Whether it’s helping explain the newest feature or sharing some tips on how to get the most out of Plakar, I’ve got your back.\nI’ll also be highlighting community contributions, answering questions, and sometimes just popping by with a fun tidbit or two about the latest happenings around Plakar Korp. Consider me your guide on this adventure of backups, snapshots, and cutting-edge tech.\nWhy You Should Stick Around # Whether you\u0026rsquo;re here to learn more about Plakar’s powerful backup features or you\u0026rsquo;re just curious about how it all works, this blog has something for you. From beginner tips to expert-level deep dives, we’re covering all bases—so you won’t want to miss out.\nJoin the Plakar Community! # If you’re enjoying what you see here and want to stay even more connected to the world of Plakar, we’ve got two great ways for you to dive deeper into our community:\n1. Join Our Discord Server 🗨️ # Want to be part of the real-time conversation? The Plakar community is growing, and our Discord server is where the magic happens! Whether you’ve got burning questions, cool ideas, or just want to chat with fellow Plakar users and developers, Discord is the place to be. Hop in and join the discussions on:\nPlakar features and updates Troubleshooting and tips Community contributions General tech talk (and maybe some memes) We’d love to have you there, so don’t be shy—click below and start chatting with us!\n👉 Join Our Discord Server\n2. Subscribe to the monthly Newsletter 📬 # Looking to stay informed but not bombarded? Our monthly newsletter is a great way to receive informations (spam free).\nSubscription form at the bottom of this page !\n","date":"19 July 2024","externalUrl":null,"permalink":"/community-news/2024-07_01/","section":"Plakar community updates","summary":"Hello, world!","title":"Introducing Plakup and the Plakup's Chronicles","type":"community-news"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]
[{"content":"","date":"19 March 2025","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"19 March 2025","externalUrl":null,"permalink":"/tags/backups/","section":"Tags","summary":"","title":"Backups","type":"tags"},{"content":"","date":"19 March 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"19 March 2025","externalUrl":null,"permalink":"/authors/gilles/","section":"Authors","summary":"","title":"Gilles","type":"authors"},{"content":"","date":"19 March 2025","externalUrl":null,"permalink":"/tags/plakar/","section":"Tags","summary":"","title":"Plakar","type":"tags"},{"content":"Hello, it\u0026rsquo;s me again !\nWe released our first beta late February but we weren’t just going to sit back and stay idle.\nI\u0026rsquo;m glad to announce that we have just released v1.0.1-beta.13, our latest version of plakar, incorporating several new features and solving some of the bugs reported by early adopters.\ngo install github.com/PlakarKorp/plakar/cmd/plakar@v1.0.1-beta.13 We hope you will give it a try and give us with feedback !\nWant to help and stay updated ?\nStar Watch Follow @PlakarKorp\nWeb UI improvements # Importer selection # So far, we have mainly displayed our ability to backup a local filesystem but plakar can backup other sources of data such as S3 buckets or remote SFTP directories.\nWe have extended the UI to allow filtering the snapshots per source type:\nIt is the first filtering out of a series of criteria matching to ease identifying relevant snapshots.\nContent browsing # We also introduced MIME-based lists allowing to restrict the browsing to specific types of files. It may seem like just a filter on top of results, but it really leverages our snapshots built-in indexes API to provide fast browsing through an overlay tree: the MIME browsing is a custom VFS with directories and files matching a given MIME. We\u0026rsquo;re essentially\u0026hellip; a database now :-)\nWe currently provide a selection menu based on MIME main types, but it could be turned into more specific browsing like restricting to jpg files rather than all images. To see it in action, simply select a main type and see how the list changes to only display relevant items:\nThis will be extended to support new ideas soon and our snapshots support holding many such overlays, so we have room for creating very nice features on top of this mechanism as we move forward.\nTimeline # When browsing a snapshot, each directory page now contains a Timeline widget:\nThis allows you to identify if that directory is also part of other snapshots, and will soon also display if they are identical or not, pinpointing snapshots that have divergences.\nThe timeline does not only work for directory but also for files, so that it is possible to identify which snapshots carry a file and soon which ones have a different version of it.\nConsidering that we are already able to provide diff unified patches between two files of different snapshots, we\u0026rsquo;re likely to provide features to ease reverting from a specific version of a file in case a snapshot shows errors were introduced.\nWe have lots of great ideas to build on top of this feature, so stay tuned for more !\nNew core features # Checkpointing # Our software can now resume a transfer should the connection to a remote repository drop. It implements it through checkpointing of transfers and provides to-the-second resume for a backup originating from the same machine: if you do a TB backup and connection drops after two hours, restarting a backup will skip over all the chunks that have already been transmitted without re-checking existence, leading to a fast resume from where it left.\nThis was introduced very recently and is a crucial component of our fully lock-less design as will be discussed in a future tech article.\nFTP exporter # I feel almost ashamed to put that here, but hey\u0026hellip; it\u0026rsquo;s a new core feature.\nWe already supported creating a backup from a remote FTP server:\n$ plakar backup ftp://ftp.eu.OpenBSD.org/pub/OpenBSD/7.6/arm64 We now can also restore a backup to a remote FTP server:\n$ plakar restore -to ftp://192.168.1.110/pub/OpenBSD/7.6/arm64 1f2a43a1 So, yes, I guess we\u0026rsquo;re back in the 90s\u0026hellip;\nThe good news is that this was unplanned and took only 20 minutes to implement, providing a good skeleton to start from if you want to write your own exporter.\nBetter S3 backend for repository # Plakar supports creating a repository in an S3 bucket, but that support was limited to AWS and Minio instances. That limitation has been lifted and we can now host repositories on a variety of S3 providers.\nSadly, we didn\u0026rsquo;t implement the fix in the S3 exporter yet because\u0026hellip; we forgot.\nThat should be done in next beta allowing us to restore to a variety of S3 providers too.\nOptimizations # We did a first round of optimizations in several places, the performance boost is already very visible, but\u0026hellip; this is the first round of many so expect moooore speed :-)\nB+tree nodes caching # It doesn\u0026rsquo;t show but plakar is actually closer to a database than it seems.\nWe don\u0026rsquo;t just scan and record a list of files, we actually record them in a B+tree data structure that is spread over multiple packfiles, with each node uncompressed and decrypted on the fly as the virtual filesystem is being browsed.\nIn previous betas, the B+tree didn\u0026rsquo;t implement any caching whatsoever and was the main point of contention. With this new beta, it comes with a nodes cache that considerably boosts performances.\nOn my machine, a backup of the Korpus repository (\u0026gt;800k files) was 6x times faster on a first run, and 10x times faster on a second run than before the nodes caching was implemented.\nConsidering our constraint not to rely on in-memory indexes for scalability, these kinds of performances boosts are impressive and bring us closer to other solutions that work fully in-memory.\nOther improvements to this are on their way so this is not the end of it !\nParallelized check and restore # In previous betas, both check and restore subcommands were written to operate sequentially, making them very slow as they would not make use of machine resources to parallelize.\nA first pass was done to parallelize them and, while the approach is still naive with no caching and no shortcuts taken (identical resources are checked twice rather than once), this already produced a x2 times boost on my machine for a Korpus check and restore.\nCheck will be improved by using a cache to avoid re-doing the same work twice, which should be very interesting on a large repository with lot of redundancy.\nRestore is already operating at mostly top speed with the target being the bottleneck, but there are still ideas to go one step further. We\u0026rsquo;ll discuss that when they have been PoC-ed or implemented.\nIn-progress deduplication # We realized that due to our parallelization, chunks could sometimes be written multiple times to a repository because a snapshot would spot them in different files, realize they are not YET in the repository but not that they were already seen in a different file.\nWe introduced a mechanism to track inflight chunk and discard duplicate requests to record them to the repository, saving a bit more disk-space and maintenance deduplication work.\nCDC attacks publication # Colin Percival from the Tarsnap project has co-authored a paper on attacks targeting CDC backup solutions:\nIn this paper, they described Parameter Extraction attacks and Post-Parametrization attacks against CDC backup solutions including Tarsnap, Restic and Borg.\nThe attacker’s general goal is to find out what files the user has uploaded, either now or in the future.\nThe Parameter Extraction attacks targets CDC implementations relying on secret values to setup the chunker: a random irreducible polynomial for Rabin (Restic), a secret base and a prime modulus for Rabin-Karp (Tarsnap) and a random table of numbers for Buzhash (Borg). We rely on FastCDC which uses a Gear table with predetermined public values, parameters are therefore already known and our security model relies on protecting data secrecy after it\u0026rsquo;s been chunked.\nThe Post-Parametrization attacks are more interesting to us as we are in this scenario. If we assume that an attacker knows the parameters of a chunker and can observe storage or traffic, they can obtain some level of information about a backup repository.\nInformation leak # When an adversary knows the chunking parameters, they can simulate the chunking process on known files to generate corresponding lists of chunk sizes. If they also have a method to identify which chunk sizes are present in a repository, they can infer with considerable confidence whether specific files exist.\nFor a simplified example, consider an attacker who chunks the /bin directory, thereby obtaining a list of chunk sizes for each file. If the attacker then monitors network traffic to my repository and observes the following sequence of chunk sizes:\n88320 106844 79623 86182 109251 122564 88059 75087 66823 82371 85138 68684 115906 119036 They could immediately conclude that the repository includes the /bin/bash program, all without needing to analyze the actual content of the chunks.\nAttacks # Basically, there are two places where someone can tap into to deduce chunk sizes: when chunks are stored and when they are retrieved.\nStorage-time # During a backup plakar builds so-called packfiles that bundle several chunks together until they reach a maximum size. Several packfiles are created in parallel, chunks from a same file are distributed across them so they are interlaced, and they are encrypted with no delimited between them as to not provide any indication of how many there are, where they begin or where they end.\nA server operator looking at a packfile will only see a string of seemingly random bytes, whereas an attacker monitoring traffic will only be able to obtain the size of a packfile itself which is fairly normalised.\nRetrieval-time # During retrieval, two things happen: chunk existence test and chunk fetch.\nThe chunk existence test is done by fetching an encrypted index of chunks, synchronizing to a local cache and resolving locally without querying the repository: it is possible to validate that a repository holds the entire data set without emitting any chunk requests to the repository.\nA server operator looking at indexes will only see a string of seemingly random bytes, whereas an attacker will only be able to obtain the size of the encrypted index which does not provide much valuable information.\nThe chunk fetch is where we potentially leak information.\nMitigations # On Friday, upon reading the paper, I came up with an immediate mitigation mechanism that I called \u0026ldquo;read and discard\u0026rdquo;.\nPut very simply, whenever fetching a chunk it added a random overhead so that the fetch would request more than the actual chunk size, and the extra bytes would be discarded client side. Because the overhead is random, a chunk that is accessed multiple times will result in different sizes for an attacker observing traffic, not only obfuscating the actual size but also the fact that it is the same chunk that was requested.\nSunday, I realized that Tarsnap implemented the Padmé scheme which is doing something similar to mine but\u0026hellip; WAY better in terms of not wasting too much data on overhead. I looked into it, adapted my read and discard to use the same approach.\nBut then decided to push it a bit further.\nInstead of applying the overhead as a padding and reading extra bytes from a base offset, I used it to create a window containing the entire chunk and used a random left shift to change the base offset. As long as the shift does not exceed the overhead, the window is guaranteed to contain the full chunk. This will produce the same size as a Padmé scheme for an attacker observing traffic, but a server operator with access to the Range request will see different offsets at each request targeting the same chunks. Of course given enough traffic they could defeat this through a statistical analysis, but this makes it harder than just having the fixed base offset and it comes for free.\nHere\u0026rsquo;s an example of it applied to a file access:\n$ ./plakar cat 2f:/private/etc/passwd \u0026gt;/dev/null off=126916 (-7) length=1085 (+17) off=116762 (-1) length=91 (+4) off=235207 (-1) length=87 (+0) off=92866 (-0) length=86 (+0) off=126413 (-2) length=523 (+13) off=93674 (-45) length=2157 (+4) off=88353 (-5) length=271 (+5) off=69422 (-1) length=187 (+0) off=80981 (-24) length=4790 (+9) This was committed today and users of our new beta will benefit from it without even noticing !\nFinal words # As we are heading to our first stable release in a few weeks, we are working hard on squashing last bugs, polishing our tool, as well as implementing some features that we consider essential for our first release.\nFeel free to hop on our discord (we\u0026rsquo;re friendly), talk to us, test and report bugs. All help is appreciated\u0026hellip;\nAlso, there\u0026rsquo;s a bunch of social media share buttons below, just saying !\n","date":"19 March 2025","externalUrl":null,"permalink":"/articles/2025-03-19/plakar-1.0.1-beta.13-out/","section":"Plakar articles","summary":"\u003cp\u003eHello, it\u0026rsquo;s me again !\u003c/p\u003e\n\u003cp\u003eWe released our first beta late February but we weren’t just going to sit back and stay idle.\u003c/p\u003e","title":"Plakar 1.0.1-beta.13 out !","type":"articles"},{"content":" Read about Plakar latest development update. ","date":"19 March 2025","externalUrl":null,"permalink":"/articles/","section":"Plakar articles","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Read about Plakar latest development update.\n\u003c/div\u003e","title":"Plakar articles","type":"articles"},{"content":"","date":"19 March 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"19 March 2025","externalUrl":null,"permalink":"/categories/technology/","section":"Categories","summary":"","title":"Technology","type":"categories"},{"content":"We were thrilled to release our first beta in late February, but we weren’t just going to sit back and rest on our laurels.\nInstead, we immediately set our sights on refining our product, incorporating valuable user feedback, and developing innovative features that would take our software to the next level.\nWant to stay updated ?\nStar Watch Follow @PlakarKorp\nNew beta available # We are excited to announce our new beta release and warmly invite you to test it!\n$ go install github.com/PlakarKorp/plakar/cmd/plakar@v1.0.0-beta.4 During our pre-beta phase, we concentrated on building the core of Plakar—ensuring reliable data storage and reconstruction. Now that these critical components are in place, we can shift our focus to enhancing the user experience with intuitive features that simplify your work.\nSince these read-only features are low-risk, we plan to roll out frequent updates with new functionalities just appearing release after release. This is the perfect opportunity for you to test them and help us shape the tool that best meets your needs.\nPreviews # Our web UI provided previews of text content with syntax highlighting and images:\nText Image In addition to this, latest beta added preview for PDF, audio and video content:\nVideo PDF This works with either a local repository or a remote one, regardless of whether it\u0026rsquo;s encrypted or not. You can easily preview data from a backup hosted on a remote, encrypted repository using a Plakar UI launched locally. The local UI fetches data in real time as it’s read and decrypts it on the fly, so there\u0026rsquo;s no need to download the entire backup or stream any bytes in cleartext.\nThis is the first in a series of new features designed to make the user interface more efficient at helping you find exactly what you need. Without giving too much away, additional exciting features are set to roll out in the coming days!\nTest improvements # Tests are crucial, and I firmly believe that developers should avoid writing tests for their own code. If I have flawed logic in mind and write both the code and its tests, we end up verifying that my bug is correctly implemented—which isn’t our goal 😄.\nPeer review is invaluable—especially given Plakar Korp’s requirement for two reviewers and our extensive experience with it. However, as a small team that frequently collaborates on the same code, we risk developing shared assumptions that might lead us all to the same flawed logic. I also prefer that our developers focus on their strengths, particularly R\u0026amp;D problem-solving, since that is an area where external help is harder to come by than testing.\nIn January, we had tests for the most critical parts of Plakar that affected storage format and could potentially lead to data corruption. However, we wanted broader coverage that also included the non-critical components. We decided to bring on an extra resource focused solely on testing, and @sayoun expressed interest, so he began working with us.\nSince then, he has diligently reviewed every piece of code we write, adding the missing tests for each package and command. This approach allows the team to concentrate on improving the software while @sayoun brings a fresh perspective to testing without being too involved in other discussions. We’re thrilled to see new tests emerging, catching bugs, and enhancing overall quality.\nMost recently, his efforts have focused on testing the CLI subcommands—the most user-visible part of Plakar—and he has already helped fix several command-line issues.\nDocumentation improvements # This effort was spearheaded by @omar-polo, who took on the heavy lifting of restructuring our documentation—converting all previous materials into a unified format and meticulously addressing every detail. The entire team also contributed by polishing the content, adding more examples, and fixing typos, with significant input from @semarie, a familiar face from the OpenBSD project that we were delighted to see around.\nAs a result, all our documentation in mandoc format now adheres to a consistent structure, enabling us to effortlessly automate the generation of Markdown versions. These versions are seamlessly displayed to users through plakar help and synchronized on our documentation site.\nOptimizations # We implemented several optimization improvements—nothing too fancy, but small wins are still wins!\nAmong these, two stand out:\n@mathieu-plak discovered and fixed an issue with our use of the binary package, which had been causing poor performance in packfile deserialization. Although the performance boost is significant, packfile deserialization is rarely used, so the improvement is only noticeable in specific scenarios. Nevertheless, this fix prompted us to review similar constructs to ensure we hadn\u0026rsquo;t made the same mistake elsewhere.\n@omar-polo enhanced certain lookups by employing an optimized scan of our B+ tree rather than a node traversal in some cases. This change significantly boosts performance when scanning all entries of a backup.\nAs a side note, we\u0026rsquo;ve begun working on deeper optimizations that are expected to deliver even bigger gains. Stay tuned for an upcoming article detailing these enhancements.\nBugfixes # Based on user feedback—especially from @b1pb1p, @ncartron, @ajacoutot, and @semarie—we have resolved minor bugs that affected SFTP support and the agent when using specific command options.\nPackaging # In January, @ajacoutot emailed me to let me know he had packaged Plakar for the OpenBSD project. At the time, we were finalizing the storage format before freezing it, so I asked him to hold off to avoid users having to trash their data when the beta was released. Now that the beta is out and the storage format is stable, he updated his port and committed it to the OpenBSD project—OpenBSD users can simply run pkg_add plakar to get started!\nOn the same day, @lbartoletti informed me on IRC that he had packaged Plakar for the FreeBSD project, meaning FreeBSD users will soon be able to run pkg add plakar.\n","date":"8 March 2025","externalUrl":null,"permalink":"/articles/2025-03-08/plakar-beta.4-and-upcoming-features/","section":"Plakar articles","summary":"\u003cp\u003eWe were thrilled to release our first beta in late February,\nbut we weren’t just going to sit back and rest on our laurels.\u003c/p\u003e","title":"Plakar beta.4 and upcoming features","type":"articles"},{"content":"Before releasing a usable version, we wanted an expert to examine our cryptographic design and confirm we hadn’t made any regrettable choices. We were delighted to have Jean-Philippe Aumasson take care of the review—a true privilege given the high level of confidence we have in his skills: he is a recognized cryptographer who created various widely-used algorithms, including ones used in plakar, and who authored great books on the topic, two of which are on my desk as I write this.\nBelow is the unedited review after of our original submission, followed by the unedited remediation review after our corrective steps. Comments are inlined in to provide clarifications where needed.\nInitial review # Summary # Plakar is a data backup solution featuring client-side encryption, and a server-side deduplication mechanism.\nThe goal of this audit is to review:\nthe soundness of the cryptography architecture the reliability of the algorithms and protocols chosen the security of the implementation the correctness of the documentation Resources provided by Plakar include:\nthe code in encryption/symmetric.go documentation in CRYPTOGRAPHY.md and README.md Our general assessment is that the current design is cryptographically sound in terms of components choice and parameters. However, we propose a number of improvements to reduce security risks, improve performance, and rely on more state-of-the-art components.\nThe 3 sections below describes\nour observations on the design our observations on the and code our review of the changes after sharing 1. and 2. Our observations don\u0026rsquo;t include any major security issue, but instead recommendations in terms of robustness and performance. The review of the changes validated the approach chosen, the choice of algorithms, and their parameters.\nDesign # Password hashing: Scrypt vs. Argon2id # We recommend switching to Argon2id for password hashing.\nCurrently password hashing is done with scrypt, with the following parameters:\nKDFParams: KDFParams{ N: 1 \u0026lt;\u0026lt; 15, R: 8, P: 1, KeyLen: 32, scrypt was developed in 2009 as one of the first memory-hard password-based hashing schemes with tunable memory. However, Argon2id was developed through the Password Hashing Competition to address some of its shortcomings, and is now recommended by modern security guidelines (such as OWASP and NIST).\nArgon2id is defined in RFC 9106. Compared to scrypt, it has\nBetter resistance to side-channel attacks More intuitive parameterization A simpler internal logic (instead of scrypt\u0026rsquo;s requirements for PBKDF2, SHA-2, ChaCha, etc.) A Go implementation of Argon2id is available in the x/crypto package.\nWe recommend parameters t = 4 and m = 256MB, for a 256 megabyte usage. If t = 4 makes hashing too slow, then use t = 3.\nnote from the developers:\nThe KDF API was refactored so that it can use Argon2Id by default and alter its parameters or switch to a different KDF should it be required.\nChunk encryption: AES-GCM vs. AES-GCM-SIV # We recommend switching to AES-GCM-SIV for chunk encryption.\nAES-GCM-SIV is a mode defined in RFC 8452 that does not rely on randomness. It produces the nonce by computing a PRF over the message to encrypt. It implies that encrypting the same message twice will produce the same ciphertext. However, if each subkey encrypts a single chunk, this is not an issue.\nAES-GCM-SIV also prevents streaming of the data hashed, therefore the whole chunk has to be stored in memory. Since data is already chunked to be streamed, and chunks are of fixed, small size (64 KB), this is not an issue.\nThat said, AES-GCM-SIV has less adoption than AES-GCM, and is not as standardized as AES-GCM. AES-GCM is fine security-wise, switching to the SIV mode would just eliminate one risk related to randomness. Depending on the business requirements and client needs, AES-GCM may be preferable (for example, if a FIPS standard is needed).\nnote from the developers:\nThe encryption API was refactored so that it can use AES-GCM-SIV by default and switch to AES-GCM should it be required.\nThe most reliable implementation of AES-GCM-SIV is in Google\u0026rsquo;s Tink package.\nNote that the Go language maintainers are planning to add AES-GCM-SIV to x/crypto.\nSubkey encryption: AES-GCM vs. AES-KW # We recommend switching to AES-KW for subkey encryption.\nCurrently subkeys are encrypted with AES-GCM. However, there is a dedicated construction for the specific problem of encrypting symmetric keys (as short, fixed size, high entropy values), namely key wrapping.\nSwitching to AES-KW would eliminate the need of repeated nonces when encrypting a large number of subkeys with the same key. Nonce being 12-byte, or 96-bit, a collision of nonces is expected after approximately 248=281,474,976,710,656 subkeys. That\u0026rsquo;s a lot of subkeys (8 petabytes worth of 32-byte keys), but at scale and over a key\u0026rsquo;s lifetime the risk may become non-negligible.\nAES-KW is defined in RFC 3394, and is standardized in NIST SP 800 38F.\nTo integrate AES-KW, we recommend the package go-aes-key-wrap.\nChecksums potential information leak # The specification writes that \u0026ldquo;Each time a chunk is produced, a checksum of the data is computed for internal purposes and recording within the snapshot itself.\u0026rdquo;\nThe checksum is a SHA-256 hash of the cleartext data. A MAC of the checksum is then used as blob ID, although the checksum seems to be used as an index.\nOur main observation is that the knowledge of a checksum (as hash of cleartext data) can allow an attacker to identify if a given piece of cleartext data is stored. Depending on the threat model, this may or may not be an issue.\nnote from the developers:\nWe forgot to explain that digests were not visible within a backup repository as they were only part of encrypted indexes. They were supposedly only available locally to the software after it had fetched and decrypted the repository state. Regardless, we figured a way to rework our lookups and adapted the codebase to work fully on MAC and no longer make use of digests, removing all potential concerns over digests.\nFurthermore, we suggest to adjust terminology to avoid misunderstandings and use the most accurate term:\n\u0026ldquo;Checksums\u0026rdquo; are generally defined as non-secure hash values designed to detect accidental errors (such as CRCs). In contrast, \u0026ldquo;hash values\u0026rdquo; or \u0026ldquo;digests\u0026rdquo;, or \u0026ldquo;fingerprints\u0026rdquo; are generally created using cryptographic hash functoins, secure against adversarial changes.\nThe function ChecksumHMAC() data is used to produce an objects.Checksum data. Here, we suggest to replated ChecksumHMAC with (for example) MAC() or ComputeMAC, as 1) HMAC is just a type of MAC (or PRF), like AES is a type of block cipher, and 2) the value computed is not a checksum, but also called MAC (message authentication code).\nCode # The proposed implementation in symmetric.go and hashing.go use reliable, Go-native implementations of cryptographic components. It uses them in a safe way, for example using strong randomness, properly initializing a nonce/IV, and so on.\nWe just have a minor observation:\nPotential deadlock # If the reader passed to DecryptStream() does not provide full chunks of data, the read operations in the goroutine could stall indefinitely. Unless the risk is really negligible, we recommend implementing a timeout to prevent denials of service.\nnote from the developers:\nThis comment prompted a review, our assessment is that our implementation will raise an error and cause DecryptStream() to fail on incomplete chunks of data.\nRemediation review # After discussion with the Plakar maintainers, we reviewed the changes performed in the documentation and code to address our recommendations, namely the following schemes as new defaults:\nUse of BLAKE3 for hashing and MAC Use of AES-GCM-SIV for chunk encryption Use of AES-KW for subkey encryption Updated doc: CRYPTOGRAPHY.md\nIn CRYPTOGRAPHY.md#current-defaults, nit:\nKEYED BLAKE3 -\u0026gt; Keyed BLAKE3 ARGON2ID -\u0026gt; Argon2id (Also in the code, s/Argon2ID/Argon2id)\nUpdated symmetric.go: /encryption/symmetric.go\nNo problem found.\nSwitch to ARGON2ID:\nPR #447: replaces the default KDF and allows plugging SCRYPT or PBKDF2 if required (not exposed)\nThe Argon2id parameters seem to be 256KB only, is that intended? I\u0026rsquo;d recommend 256MB or more.\nArgon2IDParams: \u0026amp;Argon2IDParams{ SaltSize: saltSize, Time: 4, Memory: 256 * 1024, Threads: uint8(runtime.NumCPU()), KeyLen: 32, }, note from the developers:\nConfusingly, the size is not expressed in bytes but in kilo-bytes as confirmed by the documentation at golang.org/x/crypto/argon2: \u0026ldquo;The time parameter specifies the number of passes over the memory and the memory parameter specifies the size of the memory in KiB. For example memory=64*1024 sets the memory cost to ~64 MB.\u0026rdquo;\nThe Threads parameter was also lowered to 1 in a following commit after approval by the auditor.\nSwitch to BLAKE3:\nPR #448: Allows using BLAKE3 as a hasher for our HMAC function, we switched to BLAKE3 by default instead of SHA256 in a separate commit. PR #457 described below effectively unplugs all digests to only compute HMAC.\nOK (where the B3 HMAC is replaced by keyed B3 in another PR)\nSwitch to AES256-KW:\nPR #455: split data encryption and subkey encryption, allow using AES256-KW\nOn the verification canary: AES-KW includes an integrity check, to ensure that the unwrapped key (a subkey decrypted using the passphrase-derived key) is correct. However keeping the passphrase verification canary is fine, and needed when AES-KW is not the subkey encryption scheme used.\nSwitch to AES256-GCM-SIV:\nPR #465: switch to AES256-GCM-SIV #465\nLooks good, no problem found, OK with the tink package usage.\nSwitch from digests to MAC\nPR #457: kill checksums use hmac only. No more calls to Checksum(), function was removed and we now only rely on ComputeMAC(). The command plakar digest allows to compute a digest instead of a MAC if needed, it no longer resort to digests recorded in the snapshot.\nPR #469: the type objects.Checksum was renamed to objects.MAC, and mechanical change to rename all types and variables for consistency.\nLooks good, no problem found.\nSwitch from HMAC-BLAKE3 to Keyed BLAKE3\nPR #484: Switch from HMAC-BLAKE3 to Keyed BLAKE3\nLooks good, no problem found.\n","date":"28 February 2025","externalUrl":null,"permalink":"/docs/audits/","section":"Docs","summary":"\u003cp\u003eBefore releasing a usable version,\nwe wanted an expert to examine our cryptographic design and confirm we hadn’t made any regrettable choices.\nWe were delighted to have \u003ca href=\"https://www.aumasson.jp/\" target=\"_blank\"\u003eJean-Philippe Aumasson\u003c/a\u003e take care of the review—a true privilege given the high level of confidence we have in his skills:\nhe is a recognized cryptographer who created various widely-used algorithms,\nincluding ones used in plakar,\nand who authored great books on the topic,\ntwo of which are on my desk as I write this.\u003c/p\u003e","title":"Audit of Plakar cryptography","type":"docs"},{"content":"","date":"28 February 2025","externalUrl":null,"permalink":"/docs/","section":"Docs","summary":"","title":"Docs","type":"docs"},{"content":" Listen to this article as an AI-generated podcast as you read !\nYour browser does not support the audio element. Hello!\nThe past few months have been incredibly intense as we launched Plakar Korp to support the development of plakar and other related software.\nIn just one quarter, I transitioned from working solo to collaborating with a highly talented team—all of whom I had worked with before in various contexts and knew would be a perfect fit. Not only did they catch up remarkably fast on the existing codebase, but each of them also introduced very significant improvements, which is simply astounding to me!\nNow that I can delegate even the most intricate code to trusted people, I figured it was the perfect time to step back from my code editor and write my very first blog post:\nI’m thrilled to announce that our first beta release is now available for general testing, showcasing our current state of work.\nWhat is Plakar and why are we doing it? # plakar is a free and open-source software for creating distributed and versioned backups of various data sources. It features data deduplication, compression, and encryption. Written in Golang, Plakar is developed primarily on macOS, Linux, and OpenBSD. It is designed to work on most modern Unix-like systems, with a Windows port currently in progress and set to launch soon.\nBorn out of dissatisfaction with both open-source and commercial alternatives, Plakar was built with one clear goal in mind: to provide the most advanced backup features in the easiest form possible. With no need for hacks or custom scripts, there’s no reason to procrastinate and risk data loss.\nIt can be installed and configured in seconds: for most users, creating a backup is as simple as typing plakar backup, and restoring is as straightforward as typing plakar restore. There’s simply no reason to defer setting up backups to the next day!\nWhat to expect from this beta # Beta software can be worrisome, so why would you want to try it on a backup software?\nFirst of all, the storage format stems from years of evolutionary development. It underwent months of stabilization and stress testing, and it provides various sanity checks to ensure data integrity. While you may encounter glitches in the CLI or the web UI, which are still fairly recent, the data storage itself is nowhere near beta. Your backed-up data is stored safely, and data corruptions will not go undetected.\nSecondly, you can test our beta while retaining your existing solution, and then decide if it’s worth switching when our stable release lands. You’ll be able to evaluate usability, storage efficiency, and see how it improves your workflows and resources usage.\nFinally, by testing the beta, you’ll be able to identify commands that need improvement or are missing for your use cases. This feedback helps ensure that most common use cases are fully supported when the release lands!\nThe most likely scenario is that you’ll encounter strange logs, typos in error messages, or command options that do not work as you\u0026rsquo;d intuitively expect in some cases. We will fix these issues as quickly as possible.\nState-of-the-art deduplication # To minimize data loss during an incident, it’s crucial to perform backups frequently so that the gap between the last backup and the incident is as short as possible.\nHowever, backups involve reading data, processing it, and transmitting/writing it to storage, so the frequency of backups is limited by available resources. For example, if a backup operation takes over an hour, you can’t realistically schedule hourly backups. Similarly, if you have 1TB of storage for backing up 100GB of data, the number of backups you can store depends on how efficiently each backup uses space—essentially, on how well it avoids storing redundant data.\nInefficient deduplication can lead to data being read, transmitted, and stored more times than necessary. This not only slows down the backup process but also consumes additional bandwidth and storage space, driving up overall costs. The problem is compounded in a 3-2-1 strategy, where multiple copies across different sites can significantly amplify these inefficiencies.\nHistorically, backup systems relied on making full copies of the original data. Over time, incremental backups were introduced to store only newly created or modified files. Later, the approach evolved into fixed-sized chunking, which enabled the transmission of just the modified parts of a file—provided its structure remained unchanged. Today, the most advanced method is content-defined chunking, which intelligently divides files into chunks and adapts to shifts caused by data insertions or deletions, ensuring that only the smallest possible delta is transmitted.\nPlakar builds upon our go-cdc-chunkers package, which implements state-of-the-art content-defined chunking algorithms. In our benchmarks with similar random data, our implementations not only outperformed all others by a fair margin but also provided an excellent distribution of chunk sizes:\nBenchmark_Restic_Rabin_Next-8 613.78 MB/s 1301 chunks Benchmark_Askeladdk_FastCDC_Copy-8 2091.00 MB/s 105327 chunks Benchmark_Jotfs_FastCDC_Next-8 2473.86 MB/s 1725 chunks Benchmark_Tigerwill90_FastCDC_Split-8 3112.39 MB/s 2013 chunks Benchmark_Mhofmann_FastCDC_Next-8 2078.19 MB/s 1718 chunks Benchmark_PlakarKorp_FastCDC_Copy-8 7733.47 MB/s 3647 chunks Benchmark_PlakarKorp_FastCDC_Split-8 8142.45 MB/s 3647 chunks Benchmark_PlakarKorp_FastCDC_Next-8 8149.54 MB/s 3647 chunks Benchmark_PlakarKorp_JC_Copy-8 13431.34 MB/s 4033 chunks Benchmark_PlakarKorp_JC_Split-8 13734.42 MB/s 4033 chunks Benchmark_PlakarKorp_JC_Next-8 13739.79 MB/s 4033 chunks Github user @glycerine performed independent benchmarks, testing other algorithms such as Google\u0026rsquo;s FastCDC implementation for Stadia, which we also outperformed.\nWe will continue to track advances in the field to implement state-of-the-art algorithms and provide our users with the best deduplication they can expect.\nState-of-the-art encryption # Backups often reside in cloud services or offsite storage, which might be targeted by hackers or even vulnerable to insider threats. Attackers might attempt to steal data and silently modify or delete backups without detection.\nTo limit these risks, the only solution is to rely on end-to-end encryption (E2EE) and message authentication codes (MAC) to provide privacy, authenticity, and integrity guarantees.\nE2EE encrypts data locally before it leaves your device, ensuring that only you can decrypt and read it — even your storage provider cannot access the information. This protects your data from unauthorized access throughout its lifecycle, from creation to retrieval, even if the storage system is compromised or physical disks are stolen.\nMAC enable detection of any unauthorized modifications to the data, ensuring that any attempt to alter or tamper with your backups is promptly identified.\nSecurity is a process, not a product.\n– Bruce Schneier\nCryptography was not built as an add-on feature layered on top of plakar but is an integral part of its design.\nIt relies heavily on MAC to authenticate any information stored as part of a backup, performs encryption during backup and decryption during restore without ever sharing the secret with the storage layer. It effectively performs end-to-end encryption, allowing the hosting of repositories at public cloud providers: a backup repository does not leak information regarding its content, and any tampering is detected and reported during backup checks or restore.\nIn the sample below, I blindly modified a random byte in the backup repository which tampered with a random file, and performed a check on the backup:\n$ plakar check ec6f019c repository passphrase: ec6f019c: ✓ /private/etc/afpovertcp.cfg ec6f019c: ✓ /private/etc/apache2/extra/httpd-autoindex.conf ec6f019c: ✓ /private/etc/apache2/extra/httpd-dav.conf ec6f019c: ✓ /private/etc/apache2/extra/httpd-default.conf ec6f019c: ✓ /private/etc/apache2/extra/httpd-info.conf [...] ec6f019c: ✘ 43650d9f7...: corrupted object ec6f019c: ✘ /private/etc/openldap/schema/java.schema: corrupted file [...] ec6f019c: ✓ /private/etc/zshrc ec6f019c: ✓ /private/etc/zshrc_Apple_Terminal ec6f019c: ✓ /private/etc ec6f019c: ✓ /private ec6f019c: ✓ / Despite having a good internal understanding of what we’re doing, we decided to contract a cryptographer to perform an independent review and provide suggestions for improvements. The audit confirmed that our design is sound and provided suggestions, all of which were implemented, to strengthen our approach further and make plakar future-proof from a cryptographic standpoint.\nWe will commit to relying on independent reviews from cryptography experts and follow their guidance whenever working on cryptography-related topics, including reassessing previous decisions at regular intervals to ensure we remain ahead of evolving attacks.\nA few words on performances # When evaluating the performance of backup software, we need to consider multiple dimensions that together provide a proper balance between scalability, resource utilization, and speed.\nOur challenge is to identify the optimal tradeoffs so that you achieve maximum scalability and speed while using minimal resources.\nScalability # As data grows, both in terms of size and number of objects, challenges arise: how does a solution cope with millions of objects, or even, how does it handle millions of objects in a single directory?\nWhen I first discussed this with @misterflop, he installed a widely-used commercial software to test these worst-case scenarios, and it crashed on him at the first try.\nThe same test on plakar succeeded but back then, as was common with other open-source solutions, it used in-memory structures that made it super fast—but also caused it to consume huge amounts of memory in such cases, exposing it to OOM kills. After weeks of byte-level optimizing it became clear that none of the few bytes saved here and there would enable scaling significantly.\nWe decided to implement two major changes: first, to start relying on disk offloading during backups to avoid hogging all memory; and second, to structure snapshots as a B+ tree, which allows us to spread nodes across our storage and load them on demand rather than forcing entire indexes to fit in memory.\nThe two changes required considerable work but eventually paid off and while offloading to disk cost us roughly a 30% penalty in backup speed, plakar can now scale to very large backups without requiring insane amounts of memory. We can now put some focus on optimizing speed knowing that scalability is a solved issue !\nDeduplication efficiency # While it is difficult to produce universal metrics—since efficiency depends on the type of data being backed up and its variability over time—we can affirm that plakar is highly efficient in deduplication.\nBy combining efficient compression with state-of-the-art content-defined chunking deduplication, the first backup is generally slightly smaller than the original data, and subsequent backups are considerably smaller as they essentially consist of the delta.\nIn the example below, I backed up our korpus folder ten times producing snapshots of 33GB each. The repository holds 327GB of cumulative data, however the actual repository size is only 28G which is smaller than even a single snapshot by over 15%.\n$ plakar info |grep ^Size repository passphrase: Size: 327 GB (326968934310 bytes) $ du -sh ~/.plakar 28G /Users/gilles/.plakar A 100GB directory can be backed up dozens of times in a day while the repository grows by only a few MB if changes are limited.\nThanks to this robust deduplication, use cases where frequent backups were previously unrealistic due to wasted space—or where offsite backups were prohibitively expensive because of the storage required for multiple copies—are now viable again.\nSpeed # While our initial focus was on ensuring every backup is robust and error-free, early benchmarks indicate that our solution performs fairly well even with these priorities\u0026hellip; but we can do better because fairly well is not enough:\nOur goal is to offer the best of both worlds—robust data integrity and exceptional speed.\nThe road ahead # We plan to streamline backup and restore processes by refining our algorithms, leveraging more parallel processing, and reducing any unnecessary overhead and locking. We also plan to improve caching mechanisms and fine-tuning resource allocation, aiming at a boost of performance while keeping resources consumption minimal.\nThese are all fancy words to say: we plan for faster backups and recoveries.\nThe foundations we’ve laid for correctness and scalability create a strong baseline upon which we can continuously optimize performance. As we incorporate these enhancements and you update to newer versions, you can expect even shorter backup and recovery windows.\nThis is not just claims: in prototypes with basic parallelization optimizations, integrity checks of backups obtain up to a x10 boost and restore obtains up to a x4 boost\u0026hellip; and these are with naive optimizations.\nA few words on reliability # It is impossible to guarantee that backups are never corrupted, because storage failures and bugs are bound to happen. However, we should make sure that backups do not produce corrupted data, and that data corruption happening in the storage do not go undetected.\nDeferred garbage collection # plakar operations are non-destructive by design: clients push new states that are aggregated to previous states, even the deletion of a snapshot is technically the addition of a deletion event.\nA maintenance job can be scheduled at frequent intervals to reclaim storage space by removing resources that are no longer referenced by a snapshot. Lots of effort has been poured into making this process lock-less and allow maintenance to happen in parallel to backups in progress. However, because this is the only destructive operation in a backup repository, we decided to take two safety net measures during the beta phase:\nmaintenance locking: despite the maintenance being lock-less, we are temporarily resorting to a maintenance lock preventing maintenance to happen in parallel to backups in progress. The goal is to prevent users from having to deal with corner-cases that we\u0026rsquo;re unaware of and that we\u0026rsquo;ll try to provoke through stress-testing. When we\u0026rsquo;re 100% confident, the locking can transparently go away in a new version.\ndeferred garbage collection: orphaned resources are marked as deleted but are only deleted after a grace period. During that grace period, plakar will fail lookups as if the resources had been removed for real and display an error message\u0026hellip; while keeping them at hands should a bug have creeped in. If the error message is never encountered in the next few weeks or months, this deferred garbage collection can transparently go away in a new version.\nTest Coverage # Because recent plakar development happened at a fast pace, we prioritized writing tests for the most critical components that could lead to data corruption. All of the lower layers, from storage to encryption, have unit tests integrated into our CI, which prevents code merges if a test fails.\nWork is in progress to continuously improve testing even for the upper layers, such as the CLI and subcommands, even though these components carry minimal risk of causing data corruption.\nThe plakar Korpus # We compiled a corpus of millions of objects, including text, code, binary objects, and images coming from popular code repositories. plakar is tested by performing backups of this corpus, then running integrity checks and restores, none of which are supposed to ever fail regardless of how many times they are ran.\nWhile this corpus is representative of the wide variety of data people tend to back up, it is a worst-case scenario since it contains a LOT of heterogeneous data in a single backup, making it very likely to be worse than your typical use-cases.\nIntegrity Validation # When creating backups, plakar computes a cryptographic MAC for every chunk of data as well as for entire objects. These MACs are recorded in the snapshot and used as lookup keys in the backup repository.\nThis mechanism allows it to easily validate that the stored data has not been corrupted by fetching the data and recomputing the MAC to compare it with the recorded value. This process is used during a restore, as each file has its chunks recomputed to ensure they match the records when writing back the files to a target.\nAdditionally, plakar provides a check mechanism to perform these operations without executing a full restore, allowing, for example, a laptop with 256GB of disk space to verify the integrity of a 1TB backup.\nStructures Versioning # plakar incorporates versioning for all its internal structures that interface with storage.\nThis approach ensures that sanity checks can be performed, prevents older versions from manipulating data created by newer versions, and allows new versions to reload data created by older ones.\nMore generally, it enables deduplication across different versions in use without risking corruption from misinterpreting the stored structure format.\nNow about features in this beta! # The beta comes after years of late-night development and a quarter of intense full-time teamwork.\nIt is difficult to list exhaustively all the features it brings, so let’s focus on the most notable ones and let you discover everything it has to offer by testing it!\nExtensive Built-in Documentation # Following the Unix tradition of providing manual pages for all commands, plakar offers manual pages for each of its subcommands, detailing their options and usage examples.\nThese manuals are available online and can also be accessed offline directly from the tool, ensuring you have the necessary information during an incident when internet access may be unavailable. The online documentation is synchronized with the tool\u0026rsquo;s documentation to guarantee that they are always identical and that any fixes are updated everywhere.\nThere are few things as frustrating as inaccurate or missing documentation in the middle of a stressful incident—especially when it involves potential data loss. We consider such documentation issues as critical bugs that must be fixed with the same urgency as any software defect.\nA unix-friendly CLI # Plakar has no learning curve: it mimics existing Unix-like commands to feel natural.\nYou’ll be able to run commands like plakar backup, plakar restore, plakar cat, plakar ls, plakar diff, plakar locate, or even plakar rm, so that in the event of an incident requiring fast actions you don’t need to re-discover the command line of an unfamiliar tool.\nOnce the backup repository is setup, manipulating backups becomes as natural as an everyday task.\nA user-friendly web UI # It comes with a user-friendly web UI that lets you browse, preview, and download content.\nLight mode Dark mode Over time, the web UI will progressively support all the features available in the CLI, giving users the flexibility to work in either the console or in a browser.\nMulti-backend storage layer # Plakar supports storing backups using a variety of backends.\nIt can store backups on a local or mounted filesystem, on a remote filesystem via SFTP, in an S3 bucket powered by MinIO, Vultr, Scaleway, or AWS, and even offers experimental support for databases with SQLite.\nWe will continue implementing new backends to expand the variety of storage solutions available to plakar users.\nMulti-backend importer and exporter layer # Unlike many other solutions, plakar does not focus on a single type of data source.\nIt provides an API to implement data importers and exporters, enabling it to back up data from remote sources—such as an S3 bucket—and restore them to a remote target, like a different S3 bucket, or a local directory as importers and exporters are not tied one to another and allow mixing.\nJust as with storage backends, new ones will be implemented, allowing plakar to back up more than just local filesystems while retaining the same intuitive feel and benefit from the same level of deduplication and encryption.\nCross-site synchronization # Backup repositories can be synchronized with each other in either direction.\nThe synchronization mechanism is designed to be both flexible and secure, allowing administrators to configure bidirectional replication that maintains consistent data across multiple sites. Whether you need to mirror backups for disaster recovery or adhere to regulatory constraints that dictate specific data flow directions, plakar adapts to your requirements.\nThe system optimizes data transfers by propagating only incremental changes, ensuring efficient use of bandwidth while keeping repositories in sync.\nAgent mode # It also comes with an experimental agent mode, which allows basic orchestration and scheduling of tasks in a simple infrastructure.\nThe agent mode can be used to configure specific tasks and ensure they run at given intervals, removing the need for scripting tools to control plakar.\nWant to give it a try ? # You can install and test plakar right away following these simple two steps:\nRead the simple Quickstart guide that will hold your hand and help you get started that\u0026rsquo;s all actually, no need for more :-) Want to help us ? # The best way to help us is to test plakar, report any issues you encounter so that we can improve and polish the software before the stable release, and contribute to both the documentation and code if that\u0026rsquo;s within your skillset. By testing plakar, you play a crucial role in enhancing its stability and usability, as each bug report, suggestion, or enhancement helps us refine the product and better meet the needs of our community.\nThe next best way to support us is to spread the word and share this post with your friends. Word of mouth is essential for us at this point to gain traction and popularity, as every recommendation helps build a community of engaged users invested in the project\u0026rsquo;s success.\nFinally, feel free to join our Discord server, where development takes place almost transparently every weekday (and sometimes in the evenings for night owls). There, you can chat with our community, ask both general and technical questions, and observe discussions among developers in our virtual hackrooms. You might even catch parts of our technical meetings in public vocal rooms, providing you with unique insights into our development process.\nTogether, these actions—testing, sharing, and engaging—are the pillars that help plakar evolve into a robust and user-friendly tool for everyone.\nWhat\u0026rsquo;s coming next ? # Bug fixing # We intend to squash all \u0026ldquo;blocker\u0026rdquo; bugs reported to us in preparation for an upcoming Release Candidate version. This Release Candidate will pave the way for our first stable release.\nOptimizations # First of all, we have several parallelization optimizations that we did not include initially because we focused on correctness over raw performance. Our next phase is to start parallelizing commands that are currently running sequentially.\nIn addition, we have identified several areas that require in-depth optimization, such as refining the unlocking process of our B+ tree and better caching.\nAlerting, monitoring and dashboards # We want to add support for a few features for registered users, such as availability of analytic dashboards, monitoring of backups and alerting should backups, check or synchronization of repositories do not happen at the expected intervals or fail for any reason. We are still assessing the best way to provide these features while retaining the expected privacy.\nAmazon S3 Glacier # We also want to add support for Amazon S3 Glacier to provide at least one service with Write-Once-Read-Many (WORM) capabilities.\nThis will allow users to push their backups into tamper-proof storage, ensuring that once data is written, it cannot be modified.\nMore importers ! # We want more importers to ingest data from new data sources, and we already have ideas how to move forward with this to provide the most popular ones in a relatively short timeframe\u0026hellip; but at this point no spoil !\nEnterprise version # When the RC is released, our team will split so that we always have people focusing on the community version, and people working on the enterprise features that will complement it.\nThe enterprise version will provide all the features that dont make sense to most users for small setups, but that companies rely upon for accountability, regulatory requirements or simply convenience when dealing with a large number of servers.\n","date":"26 February 2025","externalUrl":null,"permalink":"/articles/2025-02-26/plakar-beta-release/","section":"Plakar articles","summary":"\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eListen to this article as an AI-generated podcast as you read !\u003c/strong\u003e\u003c/p\u003e\n\u003ccenter\u003e\n    \u003caudio controls\u003e\n    \u003csource src=\"notebooklm.wav\" type=\"audio/wav\"\u003e\n    Your browser does not support the audio element.\n    \u003c/audio\u003e\n\u003c/center\u003e\n\u003chr\u003e\n\u003cp\u003eHello!\u003c/p\u003e","title":"Plakar beta release !","type":"articles"},{"content":"","date":"19 February 2025","externalUrl":null,"permalink":"/authors/gilles-chehade/","section":"Authors","summary":"","title":"Gilles Chehade","type":"authors"},{"content":"This page is here to show various screenshots of plakar in action !\nThe web user interface # Light mode Dark mode The CLI # ","date":"19 February 2025","externalUrl":null,"permalink":"/screenshots/","section":"Home","summary":"\u003cp\u003eThis page is here to show various screenshots of \u003ccode\u003eplakar\u003c/code\u003e in action !\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003eThe web user interface \n    \u003cdiv id=\"the-web-user-interface\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#the-web-user-interface\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003eLight mode\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003eDark mode\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/webui-snapshots-light.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/webui-snapshots-dark.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/webui-snapshot-light.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/webui-snapshot-dark.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/webui-directory-light.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/webui-directory-dark.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/webui-directory-selection-light.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/webui-directory-selection-dark.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/webui-preview-light.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/webui-preview-dark.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/webui-file-light.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/webui-file-dark.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\n\u003ch2 class=\"relative group\"\u003eThe CLI \n    \u003cdiv id=\"the-cli\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#the-cli\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/plakar-create.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\n\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/plakar-backup.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\n\n    \u003cfigure\u003e\n      \u003cimg\n        class=\"my-0 rounded-md\"\n        loading=\"lazy\"\n        src=\"/screenshots/plakar-ls.png\"\n        alt=\"\"\n      /\u003e\n      \n    \u003c/figure\u003e\n\u003c/p\u003e","title":"Screenshots","type":"page"},{"content":"","date":"11 February 2025","externalUrl":null,"permalink":"/tags/back-to-basic/","section":"Tags","summary":"","title":"Back to Basic","type":"tags"},{"content":"","date":"11 February 2025","externalUrl":null,"permalink":"/tags/backup/","section":"Tags","summary":"","title":"Backup","type":"tags"},{"content":"","date":"11 February 2025","externalUrl":null,"permalink":"/authors/plakup/","section":"Authors","summary":"","title":"Plakup","type":"authors"},{"content":"In today\u0026rsquo;s digital world, data resilience is crucial. Our Insights section is dedicated to providing expert guidance on backup best practices, disaster recovery strategies, and emerging trends in data protection.\nWhether you\u0026rsquo;re looking to understand fundamental principles like the 3-2-1 backup rule, explore advanced backup solutions, or learn how to safeguard your data from cyber threats, you\u0026rsquo;ll find valuable insights here.\nStay informed with our in-depth analyses, best practice recommendations, and industry trends to help you build a robust and secure backup strategy.\n📌 Browse our latest articles below and take your backup strategy to the next level! 🚀\n","date":"11 February 2025","externalUrl":null,"permalink":"/insights/","section":"Ready to get some insights?","summary":"\u003cp\u003eIn today\u0026rsquo;s digital world, \u003cstrong\u003edata resilience\u003c/strong\u003e is crucial. Our \u003cstrong\u003eInsights\u003c/strong\u003e section is dedicated to providing \u003cstrong\u003eexpert guidance on backup best practices, disaster recovery strategies, and emerging trends in data protection\u003c/strong\u003e.\u003c/p\u003e","title":"Ready to get some insights?","type":"insights"},{"content":"In today’s digital landscape, where downtime can cost businesses thousands of dollars per minute, having a robust disaster recovery (DR) strategy is non-negotiable. Two fundamental metrics in any business continuity plan are recovery time objective (RTO) and recovery point objective (RPO). These determine how quickly a system can recover after a failure and how much data a company is willing to lose in the process.\nA low RTO means your business aims for fast recovery, ensuring minimal service disruption, while a low RPO means you prioritize frequent backups to prevent significant data loss. Understanding these concepts is crucial for designing an effective backup strategy that aligns with your risk tolerance, budget, and operational needs.\nNote: Both RTO (and TSO, as it is sometimes referred) must be defined by the business considering business impact but taking also into account technical constraints, such as the ability to perform backups at any moment in production, as well as the capacity and cost associated with backup storage.\nWhat is Recovery Time Objective (RTO)? # Definition and Explanation # Recovery Time Objective (RTO) is the maximum acceptable amount of time that a system, application, or business process can be down after a failure before causing significant business disruption. For example, if a company sets an RTO of four hours, it means their systems must be back online within four hours of a disruption to minimize operational impact.\nFactors Affecting RTO # Business Impact Analysis (BIA): Identifying mission-critical applications and their required uptime. System Redundancy: High-availability infrastructure can minimize recovery time. Backup and Recovery Methods: Automated failover, manual recovery procedures, or real-time data replication. Disaster Recovery Testing: Frequent testing ensures realistic RTO expectations. Air Gap Quality: In extreme cases, the RTO depends on the quality of your air gap. For example, if the last magnetic tapes used for backup remain in the same data center during a fire, the RTO is significantly degraded. What is Recovery Point Objective (RPO)? # Definition and Explanation # Recovery Point Objective (RPO) defines the maximum acceptable data loss measured in time. It determines how frequently backups should be taken. For instance, an RPO of 15 minutes means that data backups occur every 15 minutes, ensuring minimal loss in the event of a failure.\nFactors Affecting RPO # Data Change Frequency: High-volume databases or rapidly changing data require a lower RPO. Backup Strategy: The method chosen, such as scheduled backups or continuous replication, has a direct impact on the RPO. Storage and Cost Constraints: Achieving a low RPO generally requires more frequent backups, increasing storage costs. Technical Capabilities: The production environment’s ability to perform backups at any given moment is a critical factor. Continuous Data Protection (CDP) # While Continuous Data Protection (CDP) is often touted for achieving near-zero RPO through real-time data replication, it is important to note that CDP does not replace traditional backups. In incidents where the replicated data itself becomes compromised, traditional backups are essential. However, CDP can contribute to an improved RTO by allowing quicker recovery of the most recent data state.\nKey Differences Between RTO and RPO # Feature Recovery Time Objective (RTO) Recovery Point Objective (RPO) Definition Maximum allowable downtime Maximum acceptable data loss Measured in Time (minutes, hours) Time (minutes, hours) Business Impact Determines system recovery speed Defines backup frequency Example A four-hour RTO means systems must be restored within four hours A 15-minute RPO means backups occur every 15 minutes Cost Factor Lower RTO generally requires higher infrastructure costs Lower RPO generally demands more frequent backups and greater storage costs Impact on Business Continuity Planning # Both RTO and RPO must be carefully aligned with a company’s risk assessment and financial constraints. Achieving shorter RTO and RPO targets requires:\nFaster Recovery Solutions: Such as hot standby systems or real-time replication, keeping in mind that these solutions must be supported by the production environment’s backup capacity. Frequent Data Backups: To reduce the window of potential data loss, balanced with the cost and storage implications. Attention to Air Gap Integrity: Ensuring that backups are stored securely, preferably isolated from the primary data center, to protect against disasters like fires or other site-specific incidents. Setting the Right RTO and RPO for Your Business # Analyzing Business Requirements # Identify Mission-Critical Applications: Determine which applications, databases, and customer portals are essential. Perform a Risk Assessment: Define acceptable levels of downtime and data loss, taking into account both business impact and technical capabilities. Consider Technical Constraints: The ability to perform backups at any moment and the costs involved in maintaining backup storage are key factors. Creating a Disaster Recovery Plan # Align Backup Frequency with RPO: Ensure that the backup schedule meets the desired RPO. Implement Automated Failover Strategies: To support a lower RTO, while being aware of the limitations of replication solutions. Ensure Robust Air Gap Practices: Avoid storing critical backup media, such as magnetic tapes, in the same data center where they might be exposed to the same risk, for example during a fire. Testing and Validating Your RTO and RPO Goals # Conduct Regular Disaster Recovery Drills: To verify that the systems can meet the defined RTO and RPO. Measure Recovery Time Performance: Compare actual recovery times against your predefined objectives. Adjust Backup Strategies as Needed: Based on test results and evolving technical capabilities or business requirements. Key Takeaways # RTO defines how quickly you recover; RPO defines how much data you lose. Shorter RTO and RPO targets require more advanced backup solutions and robust technical capabilities. The business must define RTO (and TSO) in conjunction with technical constraints, including the ability to perform immediate backups and the associated storage costs. Continuous Data Protection (CDP) supports improved RTO but does not replace traditional backups. The quality of your air gap is crucial; for example, if backup media remain in a compromised data center, your RTO can be severely impacted. Regular testing and validation are essential to ensure that recovery goals are achievable. Conclusion # Understanding RTO and RPO is essential for designing an effective business continuity plan. By carefully defining these objectives, businesses can minimize downtime, reduce data loss, and maintain customer trust. It is vital that organizations evaluate not only their risk tolerance and budget constraints but also the technical capabilities of their production environments, such as the ability to perform continuous backups and the costs of backup storage.\nInvesting in the right backup strategies and ensuring robust practices like maintaining an effective air gap are key to achieving your recovery objectives. Remember, while Continuous Data Protection (CDP) can enhance recovery times, it must be integrated with traditional backup solutions to ensure comprehensive protection against data loss and system downtime.\nA proactive disaster recovery plan is the cornerstone of long-term business stability and operational success.\n","date":"11 February 2025","externalUrl":null,"permalink":"/insights/understanding-rto-and-rpo-in-disaster-recovery/","section":"Ready to get some insights?","summary":"\u003cp\u003eIn today’s digital landscape, where \u003cstrong\u003edowntime can cost businesses thousands of dollars per minute\u003c/strong\u003e, having a robust \u003cstrong\u003edisaster recovery (DR) strategy\u003c/strong\u003e is non-negotiable. Two fundamental metrics in any \u003cstrong\u003ebusiness continuity plan\u003c/strong\u003e are \u003cstrong\u003erecovery time objective (RTO)\u003c/strong\u003e and \u003cstrong\u003erecovery point objective (RPO)\u003c/strong\u003e. These determine how quickly a system can recover after a failure and how much data a company is willing to lose in the process.\u003c/p\u003e","title":"Understanding RTO and RPO in disaster recovery","type":"insights"},{"content":"Let us get straight to the point: Amazon S3 is a phenomenal service for scalable, reliable object storage but it is not a backup solution. Sure, S3 boasts rock-solid durability and cost efficiency, but relying on it alone for backups is like trying to cover your bases with duct tape. In today’s world, where a single misclick can spell disaster, a thoughtful, multi-layered backup strategy is not just nice to have; it is absolutely essential.\nThis article digs into the reasons why S3’s native features do not suffice when it comes to safeguarding your data. We expose the design limitations of S3 for backup tasks, compare it with dedicated backup solutions, and highlight real-world scenarios that illustrate these challenges. Along the way, we share best practices, practical examples, and a few tongue-in-cheek observations about the perils of relying on S3 as your one-and-only data safeguard. If you are serious about data protection, prepare to rethink your backup strategy.\nUnderstanding S3: its strengths and its intended purpose # Amazon S3 was built to be a high-availability, scalable object storage service. It is designed to handle immense data loads for applications that demand immediate access. It is brilliant at what it does, but it was never designed to be the all-in-one solution for backups.\nWhat is Amazon S3? # At its core, S3 is an object storage system. You drop files into buckets and retrieve them whenever you need them. Its architecture is optimized for durability by distributing data across multiple physical sites. In other words, if a hard drive fails in one location, your data remains safe elsewhere. However, this setup is intended for live data access and distribution, not for managing the nuanced requirements of backups.\nS3’s features such as lifecycle policies, access control lists, and even versioning are powerful, yet they are not built for the kind of point-in-time recoveries or granular data management that a true backup solution demands. S3’s design prioritizes scale and accessibility over the precision and control that backups require. It is like using a fire hose to water your garden: effective for one purpose, but not ideal for another.\nWhy S3 is not meant to be a backup # The reality is that S3 was never designed with a backup mindset. When backing up data, you are not just storing files; you are preparing for worst-case scenarios such as accidental deletion, malicious actions, or even regional disasters. For example, S3’s eventual consistency model means that changes might not immediately reflect across all copies. In a backup scenario, that delay can turn a near-instant restore into a waiting game that could cost you dearly.\nMoreover, S3’s versioning, while useful for retrieving older copies, is not foolproof. If all versions are deleted at once, you are in trouble. Additionally, features like MFA delete make the process of removing unwanted files cumbersome, and Object Lock can restrict deletion permanently, which is not always desirable. S3 was built to store data reliably, not to manage the intricacies of a backup cycle.\nMany cloud services tout extreme durability, but it is important to remember that durability is not the same as recoverability. S3 excels at keeping your data safe from hardware failures, but it does not protect you from human error, configuration mistakes, or targeted attacks. This is why you need a backup strategy that addresses these challenges.\nThe real-world limitations of using S3 as a backup # Relying on S3 as your sole backup solution is a risky proposition. It is not that S3 loses data; rather, it is not built for the specific challenges of backup and recovery. Let us examine the practical limitations.\nData consistency and recovery challenges # Imagine you accidentally delete a critical file and expect S3’s versioning to rescue you. In theory, it might help, but S3 uses an eventual consistency model for certain operations. This means that immediately after a change, not all copies of your data may be updated. In a scenario where every second matters, this delay can lead to inconsistencies in the recovered data.\nConsider a situation where an application update inadvertently overwrites the latest version of a critical file. With S3, versioning might help, but only if you can roll back quickly and if the previous version is intact. More often than not, recovery becomes a tedious process. Recovery is not just about retrieving the latest copy of a file; it is about ensuring every piece of data is exactly where it should be, a task S3 is not optimized for.\nSecurity and compliance limitations # When protecting vital data, security is not optional; it is a mandate. Although S3 supports encryption and access control, setting these features up correctly can be challenging. A minor misconfiguration may leave your backup data exposed to malicious actors. Traditional backup solutions are designed with integrated security protocols that ensure data remains encrypted both in transit and at rest with minimal effort. S3, on the other hand, requires continuous attention to maintain proper security.\nCompliance is another concern. For industries with strict regulatory requirements, S3’s native security settings may not be sufficient. Standards such as HIPAA, GDPR, or PCI-DSS often require detailed audit trails, comprehensive access logs, and advanced encryption methods. Achieving these with S3 demands significant time and resource investment. While S3 may have impressive durability numbers, its security capabilities are limited when it comes to comprehensive backup needs.\nVersioning and deletion: a double-edged sword # S3 versioning is often viewed as a safety net for backups. In practice, however, it can work against you. Versioning allows you to retrieve older copies of your objects, but it also leaves you vulnerable if all versions are accidentally or maliciously deleted. MFA delete is intended to offer extra protection, but it can make even intentional deletions more complicated. Object Lock might seem like a solution for compliance, but it also means you can never completely remove the data if necessary.\nThe features that S3 provides to help with data recovery can sometimes hinder recovery efforts in a crisis. The backup world requires both durability and flexibility, along with rapid recovery capabilities. S3’s design falls short in this regard, often leaving you with a solution that works under ideal conditions but may fail when you need it most.\nComparing S3 with dedicated backup solutions # When it comes to data protection, you have two choices: force S3 into a role it was not designed for or use tools built specifically for backup. Here is a comparison of these options.\nTailored backup features versus S3\u0026rsquo;s generalist approach # Dedicated backup solutions are engineered for backup and recovery. They offer features such as incremental and differential backups, automated snapshotting, and rapid point-in-time restores. These systems are built with the assumption that mistakes will happen, whether due to human error or unforeseen issues, and they are designed to minimize downtime and data loss.\nS3, by contrast, is a general-purpose storage service. It reliably stores data but does not handle the nuances of backup cycles, retention policies, or quick recovery times. For instance, a dedicated backup system can restore a single file from a specific moment in time, while with S3, you may have to manually search through multiple versions. When disaster strikes, it is not as simple as instructing S3 to \u0026ldquo;roll back\u0026rdquo; and expect everything to be restored instantly.\nCost, complexity, and management overhead # At first glance, S3 may seem like a cheaper option due to its pay-as-you-go pricing. However, when you factor in the additional software, manual processes, and ongoing monitoring needed to make S3 work as a backup, the costs can quickly add up. Dedicated backup solutions come with integrated management interfaces, reporting tools, and automated recovery procedures that simplify operations and reduce the risk of human error.\nThe management overhead is not only a financial concern; it is also a matter of time and effort. Keeping track of encryption keys, version histories, and access policies in S3 can become a logistical challenge. In contrast, a dedicated backup system is designed to integrate seamlessly with your workflows, allowing you to focus on ensuring your data is restorable when you need it most.\nBest practices for a rock-solid backup strategy # No one is immune to mistakes. Fat-fingered deletions, configuration errors, and unforeseen mishaps are all part of managing data. That is why you need a backup strategy that is as layered as your overall security measures. Here are some best practices for building a robust backup system.\nEmbracing a multi-layered backup approach # Relying on a single backup method is a recipe for disaster. Instead, adopt a multi-layered strategy. S3 is excellent for storing massive amounts of data economically, but for critical data, you need multiple copies in different locations. Use local backups for rapid recovery, integrate cloud-native backup tools for continuous data protection, and consider offsite backups with other providers for additional security.\nSome organizations use S3 for archival purposes while relying on dedicated backup appliances or software for daily snapshots and rapid restores. This redundancy ensures that if one backup fails, another layer is ready to take over.\nLeveraging the right tools for the job # Not all backup tools are created equal. Choose systems that offer automated testing, granular recovery options, and seamless integration with your existing infrastructure. Whether you opt for a commercial backup solution or an open-source alternative, make sure it supports features like incremental backups, easy-to-use dashboards, and robust encryption. The goal is to create a system where every piece of data can be tracked, restored, and verified without having to perform complex maneuvers in a crisis.\nLessons from real-world case studies # Real-world experiences provide valuable lessons. Many organizations have discovered, often the hard way, that relying solely on S3 can lead to prolonged downtime and painful recovery processes. For example, one mid-sized firm experienced a major data loss due to accidental mass deletion. They mitigated the impact by integrating S3 with a dedicated backup solution, which not only reduced recovery times but also improved overall data governance. Regular testing of backup processes can reveal weaknesses before a real crisis hits and ensure that when mistakes occur, your data is safe and recoverable.\nQuick takeaways # S3 is excellent for scalable, high-availability object storage, but it is not a backup solution. S3\u0026rsquo;s eventual consistency and versioning can create significant recovery challenges. Security configurations in S3 require constant vigilance to protect sensitive backup data. Dedicated backup solutions offer granular recovery, automated testing, and true point-in-time restores. A multi-layered backup strategy that includes local, cloud, and offsite backups minimizes risk. Conclusion # In summary, while Amazon S3 is a robust platform for storing large amounts of data with impressive durability, it is not engineered to serve as a comprehensive backup solution. S3\u0026rsquo;s architecture emphasizes high availability and cost efficiency, not the nuanced demands of rapid recovery, granular version control, or robust security in backup scenarios. Relying solely on S3 for backup is similar to using a reliable delivery truck as an armored vault; it transports your data effectively but is not designed to handle every contingency.\nA thoughtful backup strategy requires multiple layers: local backups for speed, cloud backups for redundancy, and offsite solutions for additional security. Integrating dedicated backup tools alongside S3 can help prevent the issues of accidental deletions, malicious actions, and misconfigurations that could lead to catastrophic data loss. Investing in a comprehensive backup solution is essential because when it comes to protecting critical data, durability alone is not enough.\nFAQs # 1. Why is S3 not enough as a standalone backup solution?\nS3 is designed for high-availability object storage rather than the nuanced requirements of backups such as point-in-time recovery, incremental backups, or granular restoration. Its eventual consistency model may delay recovery, making it unsuitable for critical backup needs.\n2. Can S3\u0026rsquo;s versioning be used effectively for backups?\nWhile S3 versioning can help recover older copies of objects, it is not foolproof. Accidental or malicious deletion of all versions can leave you without a fallback, and features like MFA delete complicate the process further.\n3. How do dedicated backup solutions compare to using S3 alone?\nDedicated backup solutions offer automated snapshotting, incremental backups, and rapid recovery options specifically tailored for disaster scenarios. They also include robust encryption and management features that make data restoration simpler and more reliable.\n4. What is a multi-layered backup strategy?\nA multi-layered backup strategy combines various methods—local backups for fast recovery, cloud-based backups for redundancy, and offsite solutions for disaster resilience—to ensure that if one layer fails, other copies remain available.\n5. How can I integrate S3 with a dedicated backup solution?\nMany modern backup platforms provide seamless integration with S3. These solutions use S3 for cost-effective archival storage while managing real-time backups and rapid restores through specialized software. This hybrid approach leverages the strengths of S3 without exposing you to its limitations.\n","date":"10 February 2025","externalUrl":null,"permalink":"/insights/s3-is-not-a-backup---why-you-need-a-real-backup-strategy/","section":"Ready to get some insights?","summary":"\u003cp\u003eLet us get straight to the point: Amazon S3 is a phenomenal service for scalable, reliable object storage but it is not a backup solution. Sure, S3 boasts rock-solid durability and cost efficiency, but relying on it alone for backups is like trying to cover your bases with duct tape. In today’s world, where a single misclick can spell disaster, a thoughtful, multi-layered backup strategy is not just nice to have; it is absolutely essential.\u003c/p\u003e","title":"S3 is not a backup: why you need a real backup strategy","type":"insights"},{"content":"In the realm of data protection, backup and replication are two fundamental strategies employed to safeguard information. While they share the common goal of data preservation, they operate on distinct principles and serve different purposes. Understanding these differences is crucial for developing a robust data protection strategy.\nUnderstanding backup and replication # What is data backup? # Data backup involves creating copies of data at specific points in time that can be restored in the event of data loss or corruption. These backups are typically stored separately from the original data, often offsite or in the cloud, to protect against disasters. Backups can be full, incremental, or differential depending on the organization\u0026rsquo;s needs.\nWhat is data replication? # Data replication entails creating and maintaining duplicate copies of data across multiple locations or systems. This process ensures that data is continuously available and accessible even if one system fails. Replication can be synchronous, where data is copied in real time, or asynchronous, where data is copied at scheduled intervals.\nKey differences between backup and replication # Purpose and objectives # Backup: Provides restore points for recovering data after loss or corruption. Replication: Ensures continuous availability by maintaining real-time copies of data. Data consistency and recovery # Backup: Lets you restore data to a specific point in time, making it ideal for recovering from accidental deletions or corruption. Replication: Keeps copies consistent with the original but does not offer historical versions for recovery. Impact on performance # Backup: Typically scheduled during off-peak hours to minimize impact on performance. Replication: Continuously updates data, which can affect system performance, especially in high-volume environments. Use cases # Backup: Best for long-term data retention, compliance, and protection against accidental data loss. Replication: Ideal for mission-critical systems requiring high availability and rapid recovery. Complementary roles in data protection # While backup and replication serve different purposes, they are complementary components of a comprehensive data protection strategy. Combining both ensures that data is both readily available and protected against various threats.\nExample: iCloud and the difference between backup and replication # Consider the example of iCloud to illustrate the difference between replication and backup. iCloud replicates the photos on your phone to the cloud, but it does not create traditional backups of them.\nScenario 1: Accidental deletion by a child # Imagine you leave your phone unattended and your child begins to explore it. In their curiosity, they accidentally delete some important photos. Since iCloud replicates the photos in real time, these deletions are immediately reflected in your iCloud storage. In this case, the deletion is instantly replicated to iCloud, and because iCloud does not keep historical versions, the deleted photos are permanently lost. If you\u0026rsquo;re lucky, you might be able to find these photos in your trash, in the Recently Deleted section on iCloud. However, this option is only available for a limited time. If the deletion occurred several months ago, the photos may have already been permanently removed from your trash, making recovery very difficult or even impossible. In that case, if you don\u0026rsquo;t have a backup elsewhere, such as an external hard drive or another backup service, you may lose those precious memories for good.\nScenario 2: Data corruption from a third-party app # Suppose you download a third-party app to edit your photos. Although the app appears safe, it malfunctions or contains a bug that causes some of your photos to become corrupted. After syncing with iCloud, the corrupted photos are also replicated to the cloud. Replication synchronizes the corrupted data, and since iCloud does not allow you to restore previous versions of the photos, the damage is irreversible. With a backup strategy in place, you could easily restore the original, uncorrupted photos.\nScenario 3: Data loss after a breakup # Imagine a personal scenario where, after a breakup, your ex-partner who still has access to your iCloud account or your phone decides to delete all of your shared photos. Because iCloud replicates changes made on the phone, any deletions are immediately reflected on the cloud. iCloud does not provide a way to roll back the deletions, and the photos are permanently gone. However, if you had a separate backup, such as a hard drive backup or another cloud service, you could have recovered those precious memories even after this unexpected event.\nConclusion # In the world of data protection, backup remains an indispensable component. Even the most robust replication strategy cannot replace the need for backups. Replication keeps data available in real time but fails to protect against scenarios such as human error, corruption, or malicious actions. Such errors are simply mirrored across your replicated systems.\nThink of replication as a safety net that ensures continuous data access. Without backups, however, this net does not catch your mistakes. Backups serve as a safety vault that allows you to recover data to a specific point in time and prevents the irreversible loss of critical information. Relying solely on replication, without the safeguard of backups, leaves data vulnerable to irreparable damage whether due to accidental deletions, software bugs, or cyber threats.\nFor organizations and individuals alike, it is crucial to understand that replication cannot replace backups. A proper data protection strategy requires both to truly secure valuable information.\nQuick takeaways # Backup creates copies of data at specific points in time for recovery purposes. Replication maintains real-time copies of data across multiple locations for high availability. Backups are cost-effective and suitable for long-term data retention. Replication requires significant infrastructure investment and can impact system performance. Combining both strategies enhances data protection and recovery capabilities. FAQs # Can replication replace backups?\nNo, replication ensures data availability but does not provide historical recovery points like backups do.\nHow does replication affect system performance?\nContinuous data replication can consume system resources and may impact performance, especially in high-volume environments.\nIs replication more expensive than backup?\nYes, replication typically requires more infrastructure and storage, making it more costly than traditional backup solutions.\nCan replication be used for disaster recovery?\nYes, replication is a key component of disaster recovery plans, ensuring data availability in case of system failures.\nHow often should backups and replications be performed?\nBackups should be scheduled based on data change frequency and compliance requirements, while replication frequency depends on the criticality of the data and the organization\u0026rsquo;s recovery objectives.\n","date":"10 February 2025","externalUrl":null,"permalink":"/insights/why-replication-is-not-backup/","section":"Ready to get some insights?","summary":"\u003cp\u003eIn the realm of data protection, \u003cstrong\u003ebackup\u003c/strong\u003e and \u003cstrong\u003ereplication\u003c/strong\u003e are two fundamental strategies employed to safeguard information. While they share the common goal of data preservation, they operate on distinct principles and serve different purposes. Understanding these differences is crucial for developing a robust data protection strategy.\u003c/p\u003e","title":"Why replication is not backup","type":"insights"},{"content":"Data loss can happen in many ways: whether due to accidental deletion, cyberattacks, hardware failure, or even a catastrophic event like a data center fire. To protect against these risks, IT professionals have long relied on the 3-2-1 backup rule, a fundamental strategy for ensuring data resilience.\nThis article breaks down what the 3-2-1 backup rule is, why it is critical, and why replication or single-cloud backups are not enough. We also explore the types of threats it mitigates, from hacker intrusions to storage provider failures, and how to implement it effectively with proper offline or air-gapped backups.\nWhat is the 3-2-1 backup rule? # The 3-2-1 backup rule is a best-practice guideline for data redundancy and disaster recovery. It ensures that organizations maintain sufficient copies of their data to minimize the risk of total data loss.\nThe core principle of 3-2-1 # The rule dictates that you should:\nKeep at least 3 copies of your data (one primary plus two backups). Store backups on at least 2 different types of media (for example, a local disk and cloud storage, or a local NAS and tape). Ensure 1 backup copy is off-site (in a different location or cloud service, ideally offline or air-gapped). This approach guarantees that even if one or two copies are lost, a third copy remains accessible for recovery.\nExample of a proper 3-2-1 backup implementation # Let us say you run a critical business application storing important customer data:\nPrimary Copy – The data lives on your production server (for example, an on-premise storage system or S3 storage). Secondary Copy – A backup is stored on a separate NAS, another cloud storage, or a different disk-based system. Tertiary Copy (Off-Site and Air-Gapped) – A cloud backup stored in AWS Glacier with a multi-day deletion delay, or a tape backup stored in a secure facility with robotic retrieval. This ensures that even if your main server fails, your local backup is corrupted, or your cloud provider is compromised, an air-gapped copy remains protected.\nWhy is it important to implement the 3-2-1 backup rule? # A single backup is never enough because data loss comes from many unpredictable sources, including:\nHuman errors – Accidental file deletions or unintended data overwrites. Hardware failures – Disk crashes, server failures, or RAID corruption. Cyber threats – Ransomware, malware, or hacker intrusions. Administrative mistakes – Accidental database deletions or misconfigurations. Cloud service failures – Unexpected outages or accidental deletions by providers. Physical disasters – Fires, floods, earthquakes, or power failures. Rogue admins – Malicious insiders deleting backups or modifying retention policies. A multi-layered backup strategy like 3-2-1 ensures that even if one or two of these failures occur, you still have a recoverable copy of your data.\nWhy replication is not a backup # One common misconception is that replication can replace backup. This is not true. While replication is useful for availability, it does not protect against data corruption, accidental deletions, or cyberattacks.\nKey differences between backup and replication # Aspect Backup Replication Purpose Disaster recovery High availability Retention Keeps historical versions Only keeps the latest version Data corruption protection Older copies remain untouched Corruption is replicated immediately Protection from human error Can restore from a clean backup Deletes or mistakes are instantly mirrored Protection from ransomware Can recover from an old snapshot Ransomware spreads to replicated copies Why replication fails as a backup strategy # Imagine an admin accidentally deletes a critical database. If your system only uses replication:\nReplication immediately mirrors the deletion across all systems so that the data is lost everywhere. There is no historical backup to restore from, meaning you cannot go back in time. If ransomware encrypts files, the encrypted data is also replicated immediately. In contrast, a proper backup solution with versioning allows recovery from an earlier, uncorrupted state.\nRead more about this topic: Why replication is not backup?\nWhy a backup in the same cloud account is not enough # Cloud services like AWS, Google Cloud, and Azure offer native snapshots and backups. While these options seem convenient, relying solely on one cloud provider can be a serious mistake.\nThe risks of same-cloud backups # Hacker intrusions and ransomware If an attacker gains access to your cloud account, they can delete all snapshots and backups. Many cloud providers allow instant deletion of backups, making recovery difficult. Solution: Store off-site backups with multi-day deletion delays, such as AWS Glacier Vault Lock. Storage provider failures Storage provider-side failures can happen. For example, Amazon S3 experienced data loss incidents due to misconfigurations, and Google Cloud once accidentally deleted customer backups because of an internal process error. Solution: Store at least one copy on a separate cloud provider or on-premise tape storage. Account termination risks If your cloud provider suspends or terminates your account, you could lose access to both production and backup data stored in that same cloud. Solution: Store an additional copy in a different cloud provider or physical tape archive. Conclusion # The 3-2-1 backup rule remains a simple yet powerful strategy to protect against a wide range of data loss scenarios.\nReplication is not backup because it does not protect against accidental deletions, corruption, or ransomware. A single cloud backup is not enough since provider failures, rogue admins, or account terminations could lead to permanent data loss. Offline or air-gapped backups are critical. Tape storage or AWS Glacier with deletion locks ensures that backups cannot be easily deleted. By keeping multiple copies on different media, and one truly protected off-site, you ensure resilience against both human and technical failures. No matter the size of your organization, implementing a proper 3-2-1 backup strategy is essential to safeguard data against disaster.\nQuick takeaways # 3-2-1 backup rule fundamentals:\nMaintain at least three copies of your data on two different types of media, with one copy stored off-site or in an air-gapped environment. Backup vs. replication:\nReplication ensures high availability but mirrors errors and corruption immediately, making it insufficient as a standalone backup strategy. Comprehensive threat protection:\nA robust backup strategy defends against human error, hardware failures, cyberattacks (including ransomware), and physical disasters. Limitations of cloud-only backups:\nRelying solely on one cloud provider can expose your data to risks such as security breaches, misconfigurations, or account terminations. Importance of offline/air-gapped backups:\nOffline or air-gapped backups (for example, tape storage or AWS Glacier with deletion locks) are critical to prevent accidental or malicious data deletions. Ensuring data resilience:\nA multi-layered backup approach, as outlined by the 3-2-1 rule, guarantees that even if one or more copies are lost, your data remains recoverable. ","date":"10 February 2025","externalUrl":null,"permalink":"/insights/the-3-2-1-backup-rule---a-proven-strategy-for-data-protection/","section":"Ready to get some insights?","summary":"\u003cp\u003eData loss can happen in many ways: whether due to accidental deletion, cyberattacks, hardware failure, or even a catastrophic event like a \u003cstrong\u003edata center fire\u003c/strong\u003e. To protect against these risks, IT professionals have long relied on the \u003cstrong\u003e3-2-1 backup rule\u003c/strong\u003e, a fundamental strategy for ensuring data resilience.\u003c/p\u003e","title":"The 3-2-1 backup rule: A proven strategy for data protection","type":"insights"},{"content":"","date":"18 December 2024","externalUrl":null,"permalink":"/","section":"Home","summary":"","title":"Home","type":"page"},{"content":" Publisher # Plakar Simplified Joint-Stock Company (SAS) with a capital of 1,289.86 euros SIREN: 933 509 754 RCS Paris Head office: 149 avenue du Maine, 75014 Paris, France EU VAT number: FR79933509754 Host: KANDBAZ SAS, 1 rue de Stockholm, 75008 Paris, France\nContact Us # Mail: Plakar SAS, 149 avenue du Maine, 75014 Paris, France Technical Support: help@plakar.io\nAbuse # To report illegal content or if you are the victim of fraudulent use of our services, please contact us at: help@plakar.io\nIntellectual Property # This website and all its content (including data, information, photos, logos, and trademarks) are the exclusive property of Plakar SAS or its partners. Any reproduction, representation, translation, adaptation, or citation, whether partial or complete, regardless of the process or medium, is strictly prohibited except as provided by law or expressly authorized by their owner. Non-contractual photos.\nPersonal Data # You can visit our website without having to disclose your identity or provide personal information. However, we may request information from you to process an order, identify a technical support request, correspond with you, provide a subscription, or submit a job application.\n","date":"18 December 2024","externalUrl":null,"permalink":"/legal/","section":"Legal Notice","summary":"\u003ch2 class=\"relative group\"\u003ePublisher \n    \u003cdiv id=\"publisher\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#publisher\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003ePlakar\u003c/strong\u003e\n\u003cbr /\u003eSimplified Joint-Stock Company (SAS) with a capital of 1,289.86 euros\n\u003cbr /\u003eSIREN: 933 509 754 RCS Paris\n\u003cbr /\u003eHead office: 149 avenue du Maine, 75014 Paris, France\n\u003cbr /\u003eEU VAT number: FR79933509754\n\u003cbr /\u003eHost: KANDBAZ SAS, 1 rue de Stockholm, 75008 Paris, France\u003c/p\u003e","title":"Legal Notice","type":"legal"},{"content":"Hello teams\n","date":"18 December 2024","externalUrl":null,"permalink":"/team/team/","section":"Teams","summary":"\u003cp\u003eHello teams\u003c/p\u003e","title":"Team","type":"team"},{"content":"","date":"18 December 2024","externalUrl":null,"permalink":"/team/","section":"Teams","summary":"","title":"Teams","type":"team"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]